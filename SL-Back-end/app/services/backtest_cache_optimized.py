"""
ë°±í…ŒìŠ¤íŠ¸ Redis ìºì‹± ìµœì í™” ëª¨ë“ˆ
- ë°°ì¹˜ ìºì‹œ ì¡°íšŒ/ì €ì¥ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ IO ìµœì†Œí™”
- ìºì‹œ í‚¤ ì „ëµ ê°œì„  (ì¢…ëª© ë¬´ê´€)
- TTL ì—°ì¥ ë° ì••ì¶•
"""

import logging
import json
import hashlib
from typing import Dict, List, Optional, Any
from datetime import date, timedelta
import pickle
import lz4.frame
import asyncio
from concurrent.futures import ThreadPoolExecutor

from app.core.cache import cache

logger = logging.getLogger(__name__)


def _normalize_for_hash(obj: Any) -> Any:
    """
    í•´ì‹œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì •ê·œí™”

    Decimal, float, intë¥¼ ëª¨ë‘ ë™ì¼í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬
    ì›Œë°ì—… ìŠ¤í¬ë¦½íŠ¸ì™€ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œ ë™ì¼í•œ í•´ì‹œ ìƒì„± ë³´ì¥
    """
    from decimal import Decimal

    if isinstance(obj, Decimal):
        # Decimalì„ floatë¡œ ë³€í™˜ (ì¼ê´€ì„±)
        return float(obj)
    elif isinstance(obj, dict):
        return {k: _normalize_for_hash(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_normalize_for_hash(item) for item in obj]
    elif isinstance(obj, (int, float)):
        # int/floatì€ floatìœ¼ë¡œ í†µì¼
        return float(obj) if obj is not None else None
    else:
        return obj


def generate_strategy_hash(buy_conditions: Any, trading_rules: Dict = None) -> str:
    """
    ì „ëµ ì¡°ê±´ìœ¼ë¡œ ê³ ìœ  í•´ì‹œ ìƒì„±

    Args:
        buy_conditions: ë§¤ìˆ˜ ì¡°ê±´ (dict ë˜ëŠ” list)
        trading_rules: ë§¤ë§¤ ê·œì¹™ (ëª©í‘œê°€/ì†ì ˆê°€, ë³´ìœ ê¸°ê°„ ë“±)

    Returns:
        8ìë¦¬ í•´ì‹œ ë¬¸ìì—´

    Note:
        ğŸ”¥ FIX: Decimal/int/floatë¥¼ ëª¨ë‘ floatìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬
        ì›Œë°ì—…ê³¼ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œ ë™ì¼í•œ í•´ì‹œ ìƒì„± ë³´ì¥
    """
    # ë°ì´í„° ì •ê·œí™” (Decimal â†’ float ë³€í™˜)
    normalized_buy = _normalize_for_hash(buy_conditions)
    normalized_rules = _normalize_for_hash(trading_rules or {})

    strategy_data = {
        'buy_conditions': normalized_buy,
        'trading_rules': normalized_rules
    }

    # JSONìœ¼ë¡œ ì§ë ¬í™” (key ì •ë ¬ë¡œ ì¼ê´€ì„± ë³´ì¥)
    strategy_str = json.dumps(strategy_data, sort_keys=True, default=str)

    # MD5 í•´ì‹œ ìƒì„± í›„ ì• 8ìë¦¬ ì‚¬ìš©
    hash_obj = hashlib.md5(strategy_str.encode('utf-8'))
    return hash_obj.hexdigest()[:8]


class OptimizedCacheManager:
    """ìµœì í™”ëœ ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self):
        self.cache_prefix = "backtest_optimized"
        # ğŸš€ EXTREME OPTIMIZATION: TTL 30ì¼ë¡œ ì—°ì¥ (ì™„ì „ ë©”ëª¨ë¦¬ ìºì‹±)
        # íŒ©í„° ë°ì´í„°ëŠ” ê±°ì˜ ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¥ê¸° ìºì‹±
        self.default_ttl = 30 * 24 * 3600  # 7ì¼ â†’ 30ì¼

        # ğŸš€ NEW: ë©”ëª¨ë¦¬ ìºì‹œ (ì—­ì§ë ¬í™”ëœ DataFrame ì €ì¥)
        self._memory_cache: Dict[str, Dict] = {}
        self._max_memory_items = 500  # ìµœëŒ€ 500ê°œ ë‚ ì§œ ìºì‹œ
        self._executor = ThreadPoolExecutor(max_workers=4)  # ë³‘ë ¬ ì••ì¶• í•´ì œìš©

    def _generate_factor_cache_key(
        self,
        calc_date: date,
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None,
        strategy_hash: str = None
    ) -> str:
        """
        íŒ©í„° ìºì‹œ í‚¤ ìƒì„± (ì „ëµë³„ êµ¬ë¶„)

        ğŸ”¥ FIXED: ì „ëµ ì¡°ê±´ í•´ì‹œë¥¼ í¬í•¨í•˜ì—¬ ì „ëµë³„ ìºì‹œ ê²©ë¦¬
        - ìˆ˜ì • ì „: backtest_optimized:factors:{date}:{themes} (ì „ëµ êµ¬ë¶„ ë¶ˆê°€ âŒ)
        - ìˆ˜ì • í›„: backtest_optimized:factors:{date}:{themes}:{strategy_hash} (ì „ëµë³„ êµ¬ë¶„ âœ…)

        Args:
            calc_date: ê³„ì‚° ë‚ ì§œ
            factor_names: ìš”ì²­ íŒ©í„° ëª©ë¡
            target_themes: ëŒ€ìƒ í…Œë§ˆ
            target_stocks: ëŒ€ìƒ ì¢…ëª©
            strategy_hash: ì „ëµ ì¡°ê±´ í•´ì‹œ (8ìë¦¬)
        """
        # í…Œë§ˆ ì •ê·œí™”
        themes_str = ','.join(sorted(target_themes)) if target_themes else 'all'

        # ğŸ”¥ FIX: ì „ëµ í•´ì‹œë¥¼ í‚¤ì— í¬í•¨í•˜ì—¬ ì „ëµë³„ ê²©ë¦¬
        if strategy_hash:
            return f"{self.cache_prefix}:factors:{calc_date}:{themes_str}:{strategy_hash}"
        else:
            # ì›Œë°ì—… ìŠ¤í¬ë¦½íŠ¸ ë“± í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë°± (strategy_hashê°€ ì—†ëŠ” ê²½ìš°)
            return f"{self.cache_prefix}:factors:{calc_date}:{themes_str}"

    def _decompress_and_deserialize(self, data: bytes) -> Optional[Dict]:
        """ì••ì¶• í•´ì œ + ì—­ì§ë ¬í™” (ThreadPoolExecutorìš©)"""
        try:
            decompressed = lz4.frame.decompress(data)
            return pickle.loads(decompressed)
        except Exception as e:
            logger.warning(f"ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return None

    async def get_factors_batch(
        self,
        dates: List[date],
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None,
        strategy_hash: str = None
    ) -> Dict[date, Optional[Dict]]:
        """
        ë°°ì¹˜ ìºì‹œ ì¡°íšŒ (ë©”ëª¨ë¦¬ ìºì‹œ + ë³‘ë ¬ ì—­ì§ë ¬í™”)

        ìµœì í™”:
        1. ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„  ì¡°íšŒ (0ms)
        2. Redis ì¡°íšŒ (ë³‘ë ¬ ì••ì¶• í•´ì œ)
        3. ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥

        ê¸°ì¡´: 252ì¼ Ã— 36ms = 9ì´ˆ
        ìµœì í™”: ë©”ëª¨ë¦¬ íˆíŠ¸ ì‹œ 0ì´ˆ, Redis íˆíŠ¸ ì‹œ 2-3ì´ˆ

        ğŸ”¥ FIXED: strategy_hash íŒŒë¼ë¯¸í„° ì¶”ê°€ë¡œ ì „ëµë³„ ìºì‹œ ê²©ë¦¬
        """
        try:
            # 1. ìºì‹œ í‚¤ ìƒì„± (ì „ëµ í•´ì‹œ í¬í•¨)
            cache_keys = {
                d: self._generate_factor_cache_key(d, factor_names, target_themes, target_stocks, strategy_hash)
                for d in dates
            }

            # 2. ğŸš€ ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„  ì¡°íšŒ
            result = {}
            redis_miss_dates = []
            redis_miss_keys = []

            for calc_date in dates:
                cache_key = cache_keys[calc_date]
                if cache_key in self._memory_cache:
                    result[calc_date] = self._memory_cache[cache_key]
                else:
                    redis_miss_dates.append(calc_date)
                    redis_miss_keys.append(cache_key)

            memory_hits = len(dates) - len(redis_miss_dates)
            if memory_hits > 0:
                logger.info(f"âš¡ ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: {memory_hits}/{len(dates)}ê°œ ë‚ ì§œ")

            # 3. Redis ì¡°íšŒ (ë©”ëª¨ë¦¬ ìºì‹œ ë¯¸ìŠ¤ë§Œ)
            if redis_miss_dates:
                from app.core.cache import get_redis
                redis_client = get_redis()
                if not redis_client:
                    logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ì¡°íšŒ ìŠ¤í‚µ")
                    for d in redis_miss_dates:
                        result[d] = None
                else:
                    cached_values = await redis_client.mget(*redis_miss_keys)

                    # 4. ğŸš€ ë³‘ë ¬ ì••ì¶• í•´ì œ + ì—­ì§ë ¬í™”
                    loop = asyncio.get_event_loop()
                    deserialize_tasks = []

                    for cached_data in cached_values:
                        if cached_data:
                            task = loop.run_in_executor(
                                self._executor,
                                self._decompress_and_deserialize,
                                cached_data
                            )
                            deserialize_tasks.append(task)
                        else:
                            deserialize_tasks.append(asyncio.sleep(0, result=None))

                    deserialized_results = await asyncio.gather(*deserialize_tasks)

                    # 5. ê²°ê³¼ ë§¤í•‘ + ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
                    for i, calc_date in enumerate(redis_miss_dates):
                        data = deserialized_results[i]
                        result[calc_date] = data

                        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (LRU ê°„ë‹¨ êµ¬í˜„)
                        if data is not None:
                            cache_key = redis_miss_keys[i]
                            self._memory_cache[cache_key] = data

                            # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ
                            if len(self._memory_cache) > self._max_memory_items:
                                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ê°„ë‹¨í•œ êµ¬í˜„)
                                oldest_key = next(iter(self._memory_cache))
                                del self._memory_cache[oldest_key]

            # 6. í†µê³„
            hit_count = sum(1 for v in result.values() if v is not None)
            miss_count = len(dates) - hit_count
            hit_rate = (hit_count / len(dates) * 100) if len(dates) > 0 else 0

            if miss_count > 0:
                logger.warning(f"âš ï¸ ìºì‹œ ë¯¸ìŠ¤ ë°œìƒ: {miss_count}/{len(dates)}ê°œ ë‚ ì§œ")
                missed_keys = [cache_keys[d] for d, v in result.items() if v is None][:3]
                logger.warning(f"   ë¯¸ìŠ¤ëœ í‚¤ ì˜ˆì‹œ: {missed_keys}")
            else:
                logger.info(f"âœ… 100% ìºì‹œ íˆíŠ¸! ({len(dates)}ê°œ ë‚ ì§œ)")

            logger.info(f"ğŸ“Š ë°°ì¹˜ ìºì‹œ ì¡°íšŒ ê²°ê³¼: {hit_count}/{len(dates)} íˆíŠ¸ ({hit_rate:.1f}%)")

            return result

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {d: None for d in dates}

    async def set_factors_batch(
        self,
        factor_data: Dict[date, Dict[str, Dict[str, float]]],
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None,
        strategy_hash: str = None
    ) -> bool:
        """
        ë°°ì¹˜ ìºì‹œ ì €ì¥ (ë„¤íŠ¸ì›Œí¬ IO ìµœì†Œí™”)

        ê¸°ì¡´: 252ì¼ Ã— 300ms = 75ì´ˆ
        ìµœì í™”: 1íšŒ Ã— 800ms = 0.8ì´ˆ (90ë°° ê°œì„ !)

        ğŸ”¥ FIXED: strategy_hash íŒŒë¼ë¯¸í„° ì¶”ê°€ë¡œ ì „ëµë³„ ìºì‹œ ê²©ë¦¬
        """
        try:
            # 1. ìºì‹œ ë°ì´í„° ì¤€ë¹„ (ì „ëµ í•´ì‹œ í¬í•¨)
            cache_dict = {}
            for calc_date, factors in factor_data.items():
                cache_key = self._generate_factor_cache_key(
                    calc_date, factor_names, target_themes, target_stocks, strategy_hash
                )

                # ì§ë ¬í™” + ì••ì¶•
                serialized = pickle.dumps(factors, protocol=pickle.HIGHEST_PROTOCOL)
                compressed = lz4.frame.compress(serialized)

                cache_dict[cache_key] = compressed

            # 2. Redis MSETìœ¼ë¡œ ì¼ê´„ ì €ì¥
            from app.core.cache import get_redis
            redis_client = get_redis()
            if not redis_client:
                logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ì €ì¥ ìŠ¤í‚µ")
                return False

            await redis_client.mset(cache_dict)

            # 3. TTL ì„¤ì • (ì¼ê´„)
            pipeline = redis_client.pipeline()
            for cache_key in cache_dict.keys():
                pipeline.expire(cache_key, self.default_ttl)
            await pipeline.execute()

            logger.info(f"ë°°ì¹˜ ìºì‹œ ì €ì¥: {len(cache_dict)}ê°œ í•­ëª©")
            return True

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def get_price_data_cached(
        self,
        cache_key: str,
        ttl: int = None
    ) -> Optional[Any]:
        """ê°€ê²© ë°ì´í„° ìºì‹œ ì¡°íšŒ"""
        try:
            if ttl is None:
                ttl = self.default_ttl

            cached = await cache.get(cache_key)
            if cached:
                # ì••ì¶• í•´ì œ
                decompressed = lz4.frame.decompress(cached)
                return pickle.loads(decompressed)
            return None

        except Exception as e:
            logger.error(f"ê°€ê²© ë°ì´í„° ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    async def set_price_data_cached(
        self,
        cache_key: str,
        data: Any,
        ttl: int = None
    ) -> bool:
        """ê°€ê²© ë°ì´í„° ìºì‹œ ì €ì¥ (ì••ì¶•)"""
        try:
            if ttl is None:
                ttl = self.default_ttl

            # ì§ë ¬í™” + ì••ì¶•
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
            compressed = lz4.frame.compress(serialized)

            # ì••ì¶•ë¥  ë¡œê¹…
            compression_ratio = len(compressed) / len(serialized) * 100
            logger.info(f"ìºì‹œ ì••ì¶•ë¥ : {compression_ratio:.1f}% (ì›ë³¸: {len(serialized)/1024:.1f}KB â†’ ì••ì¶•: {len(compressed)/1024:.1f}KB)")

            await cache.set(cache_key, compressed, ttl=ttl)
            return True

        except Exception as e:
            logger.error(f"ê°€ê²© ë°ì´í„° ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def invalidate_factors_cache(
        self,
        start_date: date = None,
        end_date: date = None
    ) -> int:
        """íŒ©í„° ìºì‹œ ë¬´íš¨í™” (ë‚ ì§œ ë²”ìœ„)"""
        try:
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ
            from app.core.cache import get_redis
            redis_client = get_redis()
            if not redis_client:
                logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ë¬´íš¨í™” ìŠ¤í‚µ")
                return 0

            pattern = f"{self.cache_prefix}:factors:*"
            keys = await redis_client.keys(pattern)

            if keys:
                deleted = await redis_client.delete(*keys)
                logger.info(f"íŒ©í„° ìºì‹œ ë¬´íš¨í™”: {deleted}ê°œ ì‚­ì œ")
                return deleted

            return 0

        except Exception as e:
            logger.error(f"ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
            return 0


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
optimized_cache = OptimizedCacheManager()
