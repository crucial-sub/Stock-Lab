"""
ë°±í…ŒìŠ¤íŠ¸ Redis ìºì‹± ìµœì í™” ëª¨ë“ˆ
- ë°°ì¹˜ ìºì‹œ ì¡°íšŒ/ì €ì¥ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ IO ìµœì†Œí™”
- ìºì‹œ í‚¤ ì „ëµ ê°œì„  (ì¢…ëª© ë¬´ê´€)
- TTL ì—°ì¥ ë° ì••ì¶•
"""

import logging
import json
import hashlib
from typing import Dict, List, Optional, Any
from datetime import date, timedelta
import pickle
import lz4.frame

from app.core.cache import cache

logger = logging.getLogger(__name__)


class OptimizedCacheManager:
    """ìµœì í™”ëœ ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self):
        self.cache_prefix = "backtest_optimized"
        # ğŸš€ EXTREME OPTIMIZATION: TTL 30ì¼ë¡œ ì—°ì¥ (ì™„ì „ ë©”ëª¨ë¦¬ ìºì‹±)
        # íŒ©í„° ë°ì´í„°ëŠ” ê±°ì˜ ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¥ê¸° ìºì‹±
        self.default_ttl = 30 * 24 * 3600  # 7ì¼ â†’ 30ì¼

    def _generate_factor_cache_key(
        self,
        calc_date: date,
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None
    ) -> str:
        """
        íŒ©í„° ìºì‹œ í‚¤ ìƒì„± (ìµœì í™”)

        ë³€ê²½ì :
        1. ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì œì™¸ (ì¢…ëª© ë¬´ê´€ ìºì‹œ)
        2. í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ í‚¤ ê¸¸ì´ ë‹¨ì¶•
        """
        key_data = {
            'date': str(calc_date),
            'factors': sorted(factor_names),
            'themes': sorted(target_themes) if target_themes else [],
            'stocks_count': len(target_stocks) if target_stocks else 0,
        }

        # JSON ì§ë ¬í™” í›„ í•´ì‹œ
        key_str = json.dumps(key_data, sort_keys=True)
        key_hash = hashlib.md5(key_str.encode()).hexdigest()

        return f"{self.cache_prefix}:factors:{key_hash}"

    async def get_factors_batch(
        self,
        dates: List[date],
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None
    ) -> Dict[date, Optional[Dict]]:
        """
        ë°°ì¹˜ ìºì‹œ ì¡°íšŒ (ë„¤íŠ¸ì›Œí¬ IO ìµœì†Œí™”)

        ê¸°ì¡´: 252ì¼ Ã— 200ms = 50ì´ˆ
        ìµœì í™”: 1íšŒ Ã— 500ms = 0.5ì´ˆ (100ë°° ê°œì„ !)
        """
        try:
            # 1. ìºì‹œ í‚¤ ìƒì„±
            cache_keys = {
                d: self._generate_factor_cache_key(d, factor_names, target_themes, target_stocks)
                for d in dates
            }

            # 2. Redis MGETìœ¼ë¡œ ì¼ê´„ ì¡°íšŒ
            from app.core.cache import get_redis
            redis_client = get_redis()
            if not redis_client:
                logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ì¡°íšŒ ìŠ¤í‚µ")
                return {d: None for d in dates}

            redis_keys = list(cache_keys.values())
            cached_values = await redis_client.mget(*redis_keys)

            # 3. ê²°ê³¼ ë§¤í•‘
            result = {}
            for i, calc_date in enumerate(dates):
                cached_data = cached_values[i]
                if cached_data:
                    try:
                        # ì••ì¶• í•´ì œ + ì—­ì§ë ¬í™”
                        decompressed = lz4.frame.decompress(cached_data)
                        result[calc_date] = pickle.loads(decompressed)
                    except Exception as e:
                        logger.warning(f"ìºì‹œ ì—­ì§ë ¬í™” ì‹¤íŒ¨ [{calc_date}]: {e}")
                        result[calc_date] = None
                else:
                    result[calc_date] = None

            hit_count = sum(1 for v in result.values() if v is not None)
            logger.info(f"ë°°ì¹˜ ìºì‹œ ì¡°íšŒ: {hit_count}/{len(dates)} íˆíŠ¸")

            return result

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {d: None for d in dates}

    async def set_factors_batch(
        self,
        factor_data: Dict[date, Dict[str, Dict[str, float]]],
        factor_names: List[str],
        target_themes: List[str] = None,
        target_stocks: List[str] = None
    ) -> bool:
        """
        ë°°ì¹˜ ìºì‹œ ì €ì¥ (ë„¤íŠ¸ì›Œí¬ IO ìµœì†Œí™”)

        ê¸°ì¡´: 252ì¼ Ã— 300ms = 75ì´ˆ
        ìµœì í™”: 1íšŒ Ã— 800ms = 0.8ì´ˆ (90ë°° ê°œì„ !)
        """
        try:
            # 1. ìºì‹œ ë°ì´í„° ì¤€ë¹„
            cache_dict = {}
            for calc_date, factors in factor_data.items():
                cache_key = self._generate_factor_cache_key(
                    calc_date, factor_names, target_themes, target_stocks
                )

                # ì§ë ¬í™” + ì••ì¶•
                serialized = pickle.dumps(factors, protocol=pickle.HIGHEST_PROTOCOL)
                compressed = lz4.frame.compress(serialized)

                cache_dict[cache_key] = compressed

            # 2. Redis MSETìœ¼ë¡œ ì¼ê´„ ì €ì¥
            from app.core.cache import get_redis
            redis_client = get_redis()
            if not redis_client:
                logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ì €ì¥ ìŠ¤í‚µ")
                return False

            await redis_client.mset(cache_dict)

            # 3. TTL ì„¤ì • (ì¼ê´„)
            pipeline = redis_client.pipeline()
            for cache_key in cache_dict.keys():
                pipeline.expire(cache_key, self.default_ttl)
            await pipeline.execute()

            logger.info(f"ë°°ì¹˜ ìºì‹œ ì €ì¥: {len(cache_dict)}ê°œ í•­ëª©")
            return True

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def get_price_data_cached(
        self,
        cache_key: str,
        ttl: int = None
    ) -> Optional[Any]:
        """ê°€ê²© ë°ì´í„° ìºì‹œ ì¡°íšŒ"""
        try:
            if ttl is None:
                ttl = self.default_ttl

            cached = await cache.get(cache_key)
            if cached:
                # ì••ì¶• í•´ì œ
                decompressed = lz4.frame.decompress(cached)
                return pickle.loads(decompressed)
            return None

        except Exception as e:
            logger.error(f"ê°€ê²© ë°ì´í„° ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    async def set_price_data_cached(
        self,
        cache_key: str,
        data: Any,
        ttl: int = None
    ) -> bool:
        """ê°€ê²© ë°ì´í„° ìºì‹œ ì €ì¥ (ì••ì¶•)"""
        try:
            if ttl is None:
                ttl = self.default_ttl

            # ì§ë ¬í™” + ì••ì¶•
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
            compressed = lz4.frame.compress(serialized)

            # ì••ì¶•ë¥  ë¡œê¹…
            compression_ratio = len(compressed) / len(serialized) * 100
            logger.info(f"ìºì‹œ ì••ì¶•ë¥ : {compression_ratio:.1f}% (ì›ë³¸: {len(serialized)/1024:.1f}KB â†’ ì••ì¶•: {len(compressed)/1024:.1f}KB)")

            await cache.set(cache_key, compressed, ttl=ttl)
            return True

        except Exception as e:
            logger.error(f"ê°€ê²© ë°ì´í„° ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def invalidate_factors_cache(
        self,
        start_date: date = None,
        end_date: date = None
    ) -> int:
        """íŒ©í„° ìºì‹œ ë¬´íš¨í™” (ë‚ ì§œ ë²”ìœ„)"""
        try:
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ
            from app.core.cache import get_redis
            redis_client = get_redis()
            if not redis_client:
                logger.warning("Redis í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, ìºì‹œ ë¬´íš¨í™” ìŠ¤í‚µ")
                return 0

            pattern = f"{self.cache_prefix}:factors:*"
            keys = await redis_client.keys(pattern)

            if keys:
                deleted = await redis_client.delete(*keys)
                logger.info(f"íŒ©í„° ìºì‹œ ë¬´íš¨í™”: {deleted}ê°œ ì‚­ì œ")
                return deleted

            return 0

        except Exception as e:
            logger.error(f"ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
            return 0


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
optimized_cache = OptimizedCacheManager()
