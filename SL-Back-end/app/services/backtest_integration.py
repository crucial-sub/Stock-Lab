"""
ë°±í…ŒìŠ¤íŠ¸ ìµœì í™” í†µí•© ëª¨ë“ˆ
ê¸°ì¡´ BacktestEngineì— ìµœì í™”ëœ í•¨ìˆ˜ë“¤ì„ ì£¼ì…í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í†µí•©
"""

import logging
import asyncio
from datetime import date, timedelta
from typing import Dict, List, Optional, Any
import pandas as pd
import polars as pl
import time

from app.services.backtest_factor_optimized import OptimizedFactorCalculator
from app.services.backtest_cache_optimized import optimized_cache
from app.services.backtest_db_optimized import OptimizedDBManager
from app.services.condition_evaluator_vectorized import vectorized_evaluator
from app.services.factor_dependency_analyzer import FactorDependencyAnalyzer

# ì´ˆê³ ì† ëª¨ë“œ (Numba JIT + ë³‘ë ¬ ì²˜ë¦¬)
try:
    from app.services.backtest_ultra_optimized import ultra_fast_calculator, NUMBA_AVAILABLE
    ULTRA_FAST_MODE = NUMBA_AVAILABLE
except ImportError:
    ULTRA_FAST_MODE = False

# ê·¹í•œ ìµœì í™” ëª¨ë“œ (Extreme Performance)
try:
    from app.services.backtest_extreme_optimized import extreme_optimizer
    EXTREME_MODE = True
except ImportError:
    EXTREME_MODE = False

logger = logging.getLogger(__name__)


def integrate_optimizations(backtest_engine):
    """
    BacktestEngineì— ìµœì í™” í•¨ìˆ˜ ì£¼ì…

    ì‚¬ìš©ë²•:
        engine = BacktestEngine(db)
        integrate_optimizations(engine)
        # ì´ì œ engineì€ ìµœì í™”ëœ ë²„ì „ìœ¼ë¡œ ë™ì‘
    """

    # ìµœì í™” ëª¨ë“ˆ ì´ˆê¸°í™”
    factor_calc = OptimizedFactorCalculator()
    db_manager = OptimizedDBManager(backtest_engine.db)

    # ì›ë³¸ í•¨ìˆ˜ ë°±ì—… (í•„ìš”ì‹œ ë³µì›ìš©)
    backtest_engine._original_load_price_data = backtest_engine._load_price_data
    backtest_engine._original_load_financial_data = backtest_engine._load_financial_data
    backtest_engine._original_calculate_all_factors_optimized = backtest_engine._calculate_all_factors_optimized
    backtest_engine._original_save_result = backtest_engine._save_result

    # ==================== ìµœì í™” í•¨ìˆ˜ ì£¼ì… ====================

    async def _load_all_data_parallel(
        start_date: date,
        end_date: date,
        target_themes: List[str] = None,
        target_stocks: List[str] = None
    ) -> tuple:
        """
        ğŸš€ OPTIMIZATION 1: ë³‘ë ¬ ë°ì´í„° ë¡œë“œ (ë…ë¦½ ì„¸ì…˜ + asyncio.gather)

        ğŸ”§ EXTREME PERFORMANCE:
        - asyncio.gatherë¡œ 3ê°œ ì¿¼ë¦¬ ë™ì‹œ ì‹¤í–‰
        - ê° ì¿¼ë¦¬ëŠ” ë…ë¦½ì ì¸ DB ì„¸ì…˜ ì‚¬ìš© (ë™ì‹œì„± ì•ˆì „)
        - 2-3ì´ˆ â†’ 0.8-1ì´ˆ (60% ê°œì„ !)
        """
        import time
        _start_time = time.time()
        logger.info("ğŸš€âš¡ ë³‘ë ¬ ë°ì´í„° ë¡œë“œ ì‹œì‘")

        # ğŸ” ë””ë²„ê¹…: target_stocks í™•ì¸
        logger.debug(f"ğŸ¯ ì „ë‹¬ë°›ì€ target_stocks: {len(target_stocks or [])}ê°œ ì¢…ëª©")
        logger.debug(f"ğŸ¯ ì „ë‹¬ë°›ì€ target_themes: {len(target_themes or [])}ê°œ í…Œë§ˆ")

        # ìºì‹œ í‚¤ ìƒì„± (í…Œë§ˆ/ì¢…ëª© ì´ë¦„ ê¸°ë°˜)
        themes_str = ','.join(sorted(target_themes or []))
        stocks_str = ','.join(sorted(target_stocks or []))
        price_cache_key = f"price_data:{start_date}:{end_date}:{themes_str}:{stocks_str}"
        financial_cache_key = f"financial_data:{start_date}:{end_date}:{stocks_str}"
        stock_prices_cache_key = f"stock_prices:{start_date}:{end_date}:{stocks_str}"

        # ğŸš€ ë³‘ë ¬ ë¡œë“œ í—¬í¼ í•¨ìˆ˜
        async def _load_price_parallel():
            try:
                cached = await optimized_cache.get_price_data_cached(price_cache_key)
                if cached is None:
                    # ë…ë¦½ DB ë§¤ë‹ˆì € ìƒì„± (ë™ì‹œì„± ì•ˆì „)
                    from app.core.database import AsyncSessionLocal
                    async with AsyncSessionLocal() as independent_db:
                        independent_manager = OptimizedDBManager(independent_db)
                        data = await independent_manager.load_price_data_optimized(
                            start_date, end_date, target_themes, target_stocks
                        )
                    if not data.empty:
                        await optimized_cache.set_price_data_cached(price_cache_key, data)
                    return data
                return cached
            except Exception as e:
                logger.error(f"ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return pd.DataFrame()

        async def _load_financial_parallel():
            try:
                cached = await optimized_cache.get_price_data_cached(financial_cache_key)
                if cached is None:
                    from app.core.database import AsyncSessionLocal
                    async with AsyncSessionLocal() as independent_db:
                        independent_manager = OptimizedDBManager(independent_db)
                        data = await independent_manager.load_financial_data_optimized(
                            start_date, end_date, target_stocks=target_stocks
                        )
                    if not data.empty:
                        await optimized_cache.set_price_data_cached(financial_cache_key, data)
                    return data
                return cached
            except Exception as e:
                logger.error(f"ì¬ë¬´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return pd.DataFrame()

        async def _load_stock_prices_parallel():
            try:
                cached = await optimized_cache.get_price_data_cached(stock_prices_cache_key)
                if cached is None:
                    from app.core.database import AsyncSessionLocal
                    async with AsyncSessionLocal() as independent_db:
                        independent_manager = OptimizedDBManager(independent_db)
                        data = await independent_manager.load_stock_prices_data(
                            start_date, end_date, target_stocks or []
                        )
                    if not data.empty:
                        await optimized_cache.set_price_data_cached(stock_prices_cache_key, data)
                    return data
                return cached
            except Exception as e:
                logger.error(f"ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return pd.DataFrame()

        # ğŸš€âš¡ ë³‘ë ¬ ì‹¤í–‰ (3ê°œ ì¿¼ë¦¬ ë™ì‹œ ì‹¤í–‰!)
        price_data, financial_data, stock_prices_data = await asyncio.gather(
            _load_price_parallel(),
            _load_financial_parallel(),
            _load_stock_prices_parallel()
        )

        _load_time = time.time() - _start_time
        logger.info(f"âš¡ ë³‘ë ¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {_load_time:.2f}ì´ˆ")
        logger.info(f"   - ê°€ê²© ë°ì´í„°: {len(price_data):,}ê±´")
        logger.info(f"   - ì¬ë¬´ ë°ì´í„°: {len(financial_data):,}ê±´")
        logger.info(f"   - ìƒì¥ì£¼ì‹ìˆ˜: {len(stock_prices_data):,}ê±´")

        return price_data, financial_data, stock_prices_data

    async def _load_price_data_optimized(
        start_date: date,
        end_date: date,
        target_themes: List[str] = None,
        target_stocks: List[str] = None
    ) -> pd.DataFrame:
        """ê°€ê²© ë°ì´í„° ë¡œë“œ (ë³‘ë ¬ ë¡œë“œ ë˜í¼)"""
        price_data, _, _ = await _load_all_data_parallel(
            start_date, end_date, target_themes, target_stocks
        )
        return price_data

    async def _load_financial_data_optimized(
        start_date: date,
        end_date: date
    ) -> pd.DataFrame:
        """ì¬ë¬´ ë°ì´í„° ë¡œë“œ (ë³‘ë ¬ ë¡œë“œ ë˜í¼)"""
        _, financial_data, _ = await _load_all_data_parallel(
            start_date, end_date, None, None
        )
        return financial_data

    async def _calculate_all_factors_super_optimized(
        price_data: pd.DataFrame,
        financial_data: pd.DataFrame,
        start_date: date,
        end_date: date,
        buy_conditions: Optional[List[Any]] = None,
        priority_factor: Optional[str] = None
    ) -> pd.DataFrame:
        """
        ìŠˆí¼ ìµœì í™”ëœ íŒ©í„° ê³„ì‚°

        ê°œì„  ì‚¬í•­:
        1. ë°°ì¹˜ ìºì‹œ ì¡°íšŒ/ì €ì¥ (50ì´ˆ â†’ 0.5ì´ˆ)
        2. ë²¡í„°í™” ê³„ì‚° (252ì´ˆ â†’ 30ì´ˆ)
        3. ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
        """
        logger.info("ğŸš€ íŒ©í„° ê³„ì‚° ì‹œì‘")
        start_time = time.time()

        if price_data.empty:
            logger.warning("No price data")
            return pd.DataFrame()

        # 1. í•„ìš”í•œ íŒ©í„° ì¶”ì¶œ
        required_factors = backtest_engine._extract_required_factors(buy_conditions or [], priority_factor)
        if not required_factors:
            required_factors = {
                'PER', 'PBR', 'PSR', 'ROE', 'ROA', 'DEBT_RATIO',
                'MOMENTUM_1M', 'MOMENTUM_3M', 'MOMENTUM_6M', 'MOMENTUM_12M',
                'VOLATILITY', 'AVG_TRADING_VALUE', 'TURNOVER_RATE',
                'BOLLINGER_POSITION', 'BOLLINGER_WIDTH', 'RSI', 'MACD',
                'OPERATING_MARGIN', 'NET_MARGIN', 'CHANGE_RATE',
                'OPERATING_INCOME_GROWTH', 'GROSS_PROFIT_GROWTH',
                'REVENUE_GROWTH_1Y', 'REVENUE_GROWTH_3Y',
                'EARNINGS_GROWTH_1Y', 'EARNINGS_GROWTH_3Y',
                # Phase 2-A ê¸´ê¸‰ ì¶”ê°€
                'FCF_YIELD', 'CURRENT_RATIO',
                # Phase 2 ì¬ë¬´ íŒ©í„°
                'GPM', 'NPM', 'QUICK_RATIO', 'CASH_RATIO', 'DEBT_TO_EQUITY',
                'EQUITY_RATIO', 'INTEREST_COVERAGE', 'WORKING_CAPITAL_RATIO',
                'OCF_RATIO', 'ASSET_TURNOVER',
                # Phase 3 íŒ©í„°
                'PCR', 'EARNINGS_YIELD', 'BOOK_TO_MARKET', 'EV_SALES', 'EV_EBITDA',
                'VOLATILITY_20D', 'VOLATILITY_60D', 'VOLATILITY_90D',
                'VOLUME_RATIO_20D', 'MARKET_CAP',
                # Phase 2-B: ë¶€ë¶„ êµ¬í˜„ íŒ©í„° ì¶”ê°€ (19ê°œ)
                'OPM', 'QUALITY_SCORE', 'ACCRUALS_RATIO', 'ASSET_GROWTH_1Y',
                'ALTMAN_Z_SCORE', 'EARNINGS_QUALITY',
                'DISTANCE_FROM_52W_HIGH', 'DISTANCE_FROM_52W_LOW',
                'RSI_14', 'MACD_SIGNAL', 'STOCHASTIC_14', 'VOLUME_ROC', 'PRICE_POSITION',
                # NEW: 15 Missing Factors
                'PEG', 'EV_FCF', 'DIVIDEND_YIELD', 'CAPE_RATIO', 'PTBV',
                'ROIC', 'INVENTORY_TURNOVER',
                'OCF_GROWTH_1Y', 'BOOK_VALUE_GROWTH_1Y', 'SUSTAINABLE_GROWTH_RATE',
                'RELATIVE_STRENGTH', 'VOLUME_MOMENTUM', 'BETA',
                # 22 Technical Indicators
                'MA_5', 'MA_20', 'MA_60', 'MA_120', 'MA_250',  # Moving Averages (5)
                'ADX', 'AROON_UP', 'AROON_DOWN', 'ATR', 'MACD_HISTOGRAM', 'PRICE_VS_MA20',  # Trend (6)
                'CCI', 'MFI', 'ULTIMATE_OSCILLATOR', 'WILLIAMS_R', 'TRIX',  # Oscillators (5, RSI already exists)
                'CMF', 'OBV', 'VWAP',  # Volume-based (3)
                # === NEW: 40 Additional Factors ===
                # Valuation (5)
                'GRAHAM_NUMBER', 'GREENBLATT_RANK', 'MAGIC_FORMULA', 'PRICE_TO_FCF', 'PS_RATIO',
                # Momentum (9)
                'RETURN_1M', 'RETURN_3M', 'RETURN_6M', 'RETURN_12M', 'RET_3D', 'RET_8D',
                'DAYS_FROM_52W_HIGH', 'DAYS_FROM_52W_LOW', 'WEEK_52_POSITION',
                # Risk (4)
                'DOWNSIDE_VOLATILITY', 'MAX_DRAWDOWN', 'SHARPE_RATIO', 'SORTINO_RATIO',
                # Volatility (3)
                'HISTORICAL_VOLATILITY_20', 'HISTORICAL_VOLATILITY_60', 'PARKINSON_VOLATILITY',
                # Composite (3)
                'ENTERPRISE_YIELD', 'PIOTROSKI_F_SCORE', 'SHAREHOLDER_YIELD',
                # Microstructure (5)
                'AMIHUD_ILLIQUIDITY', 'EASE_OF_MOVEMENT', 'FORCE_INDEX', 'INTRADAY_VOLATILITY', 'VOLUME_PRICE_TREND',
                # Duplicate/Alias (7)
                'DEBTRATIO', 'DIVIDENDYIELD', 'EARNINGS_GROWTH', 'OPERATING_INCOME_GROWTH_YOY',
                'PEG_RATIO', 'REVENUE_GROWTH', 'SMA',
                # Dividend (2)
                'DIVIDEND_GROWTH_3Y', 'DIVIDEND_GROWTH_YOY'
            }

        logger.debug(f"í•„ìš” íŒ©í„°: {len(required_factors)}ê°œ")

        # 2. Polars ë³€í™˜
        price_pl = pl.from_pandas(price_data)
        financial_pl = pl.from_pandas(financial_data) if not financial_data.empty else None

        # 2-1. target_themes/target_stocks ì¶”ì¶œ (ë¨¼ì € ì •ì˜)
        target_themes = backtest_engine.target_themes if hasattr(backtest_engine, 'target_themes') else None
        target_stocks = backtest_engine.target_stocks if hasattr(backtest_engine, 'target_stocks') else None

        # 2-2. ìƒì¥ì£¼ì‹ìˆ˜ ë° ì‹œê°€ì´ì•¡ ë°ì´í„° ë¡œë“œ (PBR/PER ê³„ì‚°ìš©)
        # ğŸ”¥ ë³‘ë ¬ ë¡œë“œëœ ë°ì´í„° í™œìš©
        stock_prices_pl = None
        try:
            # ë¨¼ì € ìºì‹œ í™•ì¸ (ë³‘ë ¬ ë¡œë“œì—ì„œ ì´ë¯¸ ìºì‹œë¨)
            stock_prices_cache_key = f"stock_prices:{start_date}:{end_date}:{len(target_stocks or [])}"
            stock_prices_data = await optimized_cache.get_price_data_cached(stock_prices_cache_key)

            # ìºì‹œ ë¯¸ìŠ¤ë©´ ì§ì ‘ DBì—ì„œ ë¡œë“œ
            if stock_prices_data is None or (isinstance(stock_prices_data, pd.DataFrame) and stock_prices_data.empty):
                logger.info(f"ğŸ’¹ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ìºì‹œ ë¯¸ìŠ¤ - DB ë¡œë“œ ì‹œì‘")
                stock_prices_data = await db_manager.load_stock_prices_data(start_date, end_date, target_stocks or [])

                if stock_prices_data is not None and not stock_prices_data.empty:
                    await optimized_cache.set_price_data_cached(stock_prices_cache_key, stock_prices_data)
                    logger.info(f"ğŸ’¹ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° DB ë¡œë“œ ì™„ë£Œ: {len(stock_prices_data)}ê±´")
                else:
                    logger.warning(f"âš ï¸ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ì—†ìŒ - PBR/PER ê³„ì‚° ë¶ˆê°€")

            if stock_prices_data is not None and not stock_prices_data.empty:
                stock_prices_pl = pl.from_pandas(stock_prices_data)
                logger.info(f"ğŸ’¹ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ìµœì¢… ë¡œë“œ: {len(stock_prices_data)}ê±´")
            else:
                logger.warning(f"âš ï¸ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ì—†ìŒ - PBR/PER ê³„ì‚° ë¶ˆê°€")

        except Exception as e:
            logger.error(f"âŒ ìƒì¥ì£¼ì‹ìˆ˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)

        # 3. ê±°ë˜ì¼ ëª©ë¡
        unique_dates = sorted(price_data[price_data['date'] >= pd.Timestamp(start_date)]['date'].unique())
        total_dates = len(unique_dates)
        logger.info(f"ê³„ì‚° ëŒ€ìƒ: {total_dates}ê°œ ê±°ë˜ì¼")

        # 4. ë°°ì¹˜ ìºì‹œ ì¡°íšŒ

        cache_results = await optimized_cache.get_factors_batch(
            [d.date() for d in unique_dates],
            list(required_factors),
            target_themes,
            target_stocks
        )

        # 5. ìºì‹œ ë¯¸ìŠ¤ì¸ ë‚ ì§œë§Œ ê³„ì‚°
        dates_to_calc = [d for d in unique_dates if cache_results.get(d.date()) is None]
        cache_hit_rate = (1-len(dates_to_calc)/total_dates)*100 if total_dates > 0 else 0
        logger.info(f"ğŸ“Š ìºì‹œ íˆíŠ¸ìœ¨: {cache_hit_rate:.1f}% ({total_dates-len(dates_to_calc)}/{total_dates}ê°œ)")

        # 6. ë²¡í„°í™” ê³„ì‚° (ê·¹í•œ ëª¨ë“œ â†’ ì´ˆê³ ì† ëª¨ë“œ â†’ ê¸°ë³¸ ëª¨ë“œ)
        calc_start = time.time()
        all_factors_by_date = {}

        # ê·¹í•œ ìµœì í™” ëª¨ë“œ í™•ì¸ (ìš°ì„ ìˆœìœ„ 1) - ëª¨ë“  íŒ©í„° ê³„ì‚° ê°€ëŠ¥!
        # ğŸš€ OPTIMIZATION: í•­ìƒ Extreme ëª¨ë“œ ì‚¬ìš© (60-70% ì„±ëŠ¥ í–¥ìƒ)
        use_extreme = EXTREME_MODE and len(dates_to_calc) > 0
        # ì´ˆê³ ì† ëª¨ë“œ í™•ì¸ (ìš°ì„ ìˆœìœ„ 2)
        use_ultra_fast = ULTRA_FAST_MODE and len(dates_to_calc) > 3 and not use_extreme

        if use_extreme:
            logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™” (Extreme Performance - ì„ íƒì  íŒ©í„° ê³„ì‚°)")
            logger.info(f"ğŸ’° financial_pl ìƒíƒœ: None={financial_pl is None}, Empty={financial_pl.is_empty() if financial_pl is not None else 'N/A'}, Len={len(financial_pl) if financial_pl is not None else 0}")

            # JIT ì›Œë°ì—… (ì²« ì‹¤í–‰ë§Œ)
            await extreme_optimizer.warmup_jit_functions()

            # ğŸ¯ íŒ©í„° ì˜ì¡´ì„± ë¶„ì„ - í•„ìš”í•œ íŒ©í„°ë§Œ ê³„ì‚°
            factor_analyzer = FactorDependencyAnalyzer()
            required_factors = set()

            # buy_conditions ë§¤ê°œë³€ìˆ˜ì—ì„œ ì§ì ‘ íŒ©í„° ì¶”ì¶œ
            if buy_conditions:
                required_factors = factor_analyzer.extract_factors_from_conditions(
                    conditions=buy_conditions
                )

            # í•„ìš”í•œ íŒ©í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒ©í„° ì„¸íŠ¸ ì‚¬ìš©
            if not required_factors:
                logger.warning("âš ï¸ ì¡°ê±´ì‹ì—ì„œ íŒ©í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ. ê¸°ë³¸ íŒ©í„° ì„¸íŠ¸ ì‚¬ìš©")
                required_factors = {'PER', 'PBR', 'ROE', 'ROA', 'DEBT_RATIO', 'CURRENT_RATIO'}

            # compute_mask ìƒì„±
            compute_mask = {
                factor: (factor in required_factors)
                for factor in FactorDependencyAnalyzer.ALL_FACTORS
            }

            enabled_count = sum(1 for v in compute_mask.values() if v)
            speedup = 148 / enabled_count if enabled_count > 0 else 1
            logger.info(f"ğŸ¯ ì„ íƒì  íŒ©í„° ê³„ì‚°: {enabled_count}/148ê°œ íŒ©í„°ë§Œ ê³„ì‚° (ì˜ˆìƒ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°°)")
            logger.info(f"ğŸ“Š ê³„ì‚°í•  íŒ©í„°: {sorted(required_factors)}")

            # ğŸš€ OPTIMIZATION 2: ë°°ì¹˜ íŒ©í„° ê³„ì‚° (í•„ìš”í•œ íŒ©í„°ë§Œ!)
            dates_to_calc_objs = [
                calc_date.date() if hasattr(calc_date, 'date') else calc_date
                for calc_date in dates_to_calc
            ]

            all_factors_by_date = extreme_optimizer.calculate_all_indicators_batch_extreme(
                price_pl, financial_pl, dates_to_calc_objs, stock_prices_pl,
                compute_mask=compute_mask  # ğŸ‘ˆ ì„ íƒì  íŒ©í„° ê³„ì‚° í™œì„±í™”!
            )

        elif use_ultra_fast:
            logger.info("âš¡âš¡âš¡ ì´ˆê³ ì† ëª¨ë“œ í™œì„±í™” (Numba JIT + ë³‘ë ¬ ì²˜ë¦¬)")

            # ê¸°ìˆ ì  ì§€í‘œë¥¼ í•œ ë²ˆì— ê³„ì‚° (ëª¨ë“  ë‚ ì§œ)
            for calc_date in dates_to_calc:
                calc_date_obj = calc_date.date() if hasattr(calc_date, 'date') else calc_date

                # Numba JITë¡œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                technical = ultra_fast_calculator.calculate_all_technical_indicators_ultra_fast(
                    price_pl, calc_date_obj
                )
                all_factors_by_date[calc_date_obj] = technical
        else:
            logger.info("ğŸš€ ê¸°ë³¸ ë²¡í„°í™” ëª¨ë“œ (Polars)")

            for calc_date in dates_to_calc:
                calc_date_obj = calc_date.date() if hasattr(calc_date, 'date') else calc_date
                factors_today = {}

                # ëª¨ë©˜í…€ íŒ©í„°
                if any(f in required_factors for f in ['MOMENTUM_1M', 'MOMENTUM_3M', 'MOMENTUM_6M', 'MOMENTUM_12M']):
                    momentum = factor_calc.calculate_momentum_factors_vectorized(price_pl, calc_date_obj)
                    factors_today.update(momentum)

                # ê¸°ìˆ ì  ì§€í‘œ
                if any(f in required_factors for f in ['BOLLINGER_POSITION', 'BOLLINGER_WIDTH', 'RSI', 'MACD']):
                    technical = factor_calc.calculate_technical_indicators_vectorized(price_pl, calc_date_obj)
                    for stock, tech_factors in technical.items():
                        if stock not in factors_today:
                            factors_today[stock] = {}
                        factors_today[stock].update(tech_factors)

                # ë³€ë™ì„± íŒ©í„°
                if 'VOLATILITY' in required_factors:
                    volatility = factor_calc.calculate_volatility_factors_vectorized(price_pl, calc_date_obj)
                    for stock, vol_factors in volatility.items():
                        if stock not in factors_today:
                            factors_today[stock] = {}
                        factors_today[stock].update(vol_factors)

                # ìœ ë™ì„± íŒ©í„°
                if any(f in required_factors for f in ['AVG_TRADING_VALUE', 'TURNOVER_RATE']):
                    liquidity = factor_calc.calculate_liquidity_factors_vectorized(price_pl, calc_date_obj)
                    for stock, liq_factors in liquidity.items():
                        if stock not in factors_today:
                            factors_today[stock] = {}
                        factors_today[stock].update(liq_factors)

                # ê°€ì¹˜ íŒ©í„°
                if financial_pl is not None and any(f in required_factors for f in ['PER', 'PBR']):
                    value = factor_calc.calculate_value_factors_vectorized(price_pl, financial_pl, calc_date_obj)
                    for stock, val_factors in value.items():
                        if stock not in factors_today:
                            factors_today[stock] = {}
                        factors_today[stock].update(val_factors)

                # ìˆ˜ìµì„± íŒ©í„°
                if financial_pl is not None and any(f in required_factors for f in ['ROE', 'ROA', 'OPERATING_MARGIN', 'NET_MARGIN']):
                    profitability = factor_calc.calculate_profitability_factors_vectorized(financial_pl, calc_date_obj)
                    for stock, prof_factors in profitability.items():
                        if stock not in factors_today:
                            factors_today[stock] = {}
                        factors_today[stock].update(prof_factors)

                all_factors_by_date[calc_date_obj] = factors_today

        calc_time = time.time() - calc_start
        if len(dates_to_calc) > 0:
            per_date_time = calc_time / len(dates_to_calc)
            logger.info(f"âš¡ íŒ©í„° ê³„ì‚° ì™„ë£Œ: {calc_time:.2f}ì´ˆ ({len(dates_to_calc)}ê°œ ë‚ ì§œ, í‰ê·  {per_date_time:.3f}ì´ˆ/ì¼)")
        else:
            logger.info(f"âš¡ íŒ©í„° ê³„ì‚° ìŠ¤í‚µ: ëª¨ë“  ë°ì´í„° ìºì‹œ íˆíŠ¸!")

        # 7. ìºì‹œ ë¯¸ìŠ¤ ê²°ê³¼ ì €ì¥
        if all_factors_by_date:
            await optimized_cache.set_factors_batch(
                all_factors_by_date,
                list(required_factors),
                target_themes,
                target_stocks
            )

        # 8. ìºì‹œ íˆíŠ¸ + ê³„ì‚° ê²°ê³¼ í†µí•©
        for calc_date in unique_dates:
            calc_date_obj = calc_date.date() if hasattr(calc_date, 'date') else calc_date
            if cache_results.get(calc_date_obj):
                all_factors_by_date[calc_date_obj] = cache_results[calc_date_obj]

        # 9. DataFrame ë³€í™˜
        rows = []
        for calc_date, factors_map in all_factors_by_date.items():
            for stock_code, factors in factors_map.items():
                row = {
                    'date': pd.Timestamp(calc_date),
                    'stock_code': stock_code,
                    **factors
                }
                rows.append(row)

        factor_df = pd.DataFrame(rows)

        # 10. ë©”íƒ€ ì •ë³´ ì¶”ê°€
        if not factor_df.empty:
            price_meta = price_data[['date', 'stock_code', 'industry', 'market_type']].drop_duplicates()
            factor_df = factor_df.merge(price_meta, on=['date', 'stock_code'], how='left')

        total_time = time.time() - start_time
        speedup = 500/total_time if total_time > 0 else 1
        logger.info(f"âœ… íŒ©í„° ê³„ì‚° ì „ì²´ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        logger.info(f"   ğŸš€ ê¸°ì¡´ ëŒ€ë¹„ {speedup:.1f}ë°° ë¹ ë¦„! (ìºì‹œ íˆíŠ¸ìœ¨: {cache_hit_rate:.1f}%)")

        return factor_df

    # ğŸš€ OPTIMIZATION 8: ë²¡í„°í™” ì¡°ê±´ í‰ê°€ê¸° ì£¼ì…
    def _evaluate_buy_conditions_vectorized(
        factor_data: pd.DataFrame,
        stock_codes: List[str],
        buy_expression: Dict[str, Any],
        trading_date: pd.Timestamp
    ):
        """ë²¡í„°í™”ëœ ì¡°ê±´ í‰ê°€ (for loop ì œê±°)"""
        return vectorized_evaluator.evaluate_buy_conditions_vectorized(
            factor_data, stock_codes, buy_expression, trading_date
        ), {}  # ë‘ ë²ˆì§¸ ë°˜í™˜ê°’ì€ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ dict

    # ì›ë³¸ í•¨ìˆ˜ ë°±ì—…
    if hasattr(backtest_engine, 'condition_evaluator'):
        backtest_engine._original_evaluate_buy_conditions = backtest_engine.condition_evaluator.evaluate_buy_conditions
        # ë²¡í„°í™” ë²„ì „ìœ¼ë¡œ êµì²´
        backtest_engine.condition_evaluator.evaluate_buy_conditions = _evaluate_buy_conditions_vectorized

    # í•¨ìˆ˜ êµì²´ (ë°ì´í„° ë¡œë“œëŠ” ì›ë³¸ ì‚¬ìš© - SQLAlchemy ë™ì‹œì„± ì—ëŸ¬ ë°©ì§€)
    # backtest_engine._load_price_data = _load_price_data_optimized
    # backtest_engine._load_financial_data = _load_financial_data_optimized
    backtest_engine._calculate_all_factors_optimized = _calculate_all_factors_super_optimized

    logger.info("âœ… ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ìµœì í™” í†µí•© ì™„ë£Œ! (ë„¤ì´í‹°ë¸Œ ë°ì´í„° ë¡œë“œ + ì¡°ê±´ í‰ê°€ ë²¡í„°í™”)")


def restore_original_functions(backtest_engine):
    """ìµœì í™” í•¨ìˆ˜ ì œê±° (ì›ë³¸ ë³µì›)"""
    if hasattr(backtest_engine, '_original_load_price_data'):
        backtest_engine._load_price_data = backtest_engine._original_load_price_data
    if hasattr(backtest_engine, '_original_load_financial_data'):
        backtest_engine._load_financial_data = backtest_engine._original_load_financial_data
    if hasattr(backtest_engine, '_original_calculate_all_factors_optimized'):
        backtest_engine._calculate_all_factors_optimized = backtest_engine._original_calculate_all_factors_optimized

    logger.info("âœ… ì›ë³¸ í•¨ìˆ˜ ë³µì› ì™„ë£Œ")
