"""
ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ
ì‹¤í–‰ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ìºì‹œ íˆíŠ¸ìœ¨ ë“±ì„ ì¸¡ì •í•˜ê³  ë¡œê¹…
"""

import time
import logging
from typing import Dict, Optional, Any
from datetime import datetime
import json
import psutil

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.metrics = {
            # ì‹œê°„ ë©”íŠ¸ë¦­ (ì´ˆ ë‹¨ìœ„)
            'data_load_time': 0.0,
            'factor_calc_time': 0.0,
            'simulation_time': 0.0,
            'save_time': 0.0,
            'total_time': 0.0,

            # ìºì‹œ ë©”íŠ¸ë¦­
            'cache_hits': 0,
            'cache_misses': 0,
            'cache_hit_rate': 0.0,

            # DB ë©”íŠ¸ë¦­
            'db_queries': 0,
            'db_query_time': 0.0,

            # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ (GB ë‹¨ìœ„)
            'memory_start': 0.0,
            'memory_peak': 0.0,
            'memory_current': 0.0,

            # CPU ë©”íŠ¸ë¦­ (%)
            'cpu_peak': 0.0,
            'cpu_average': 0.0,

            # ë°ì´í„° ë³¼ë¥¨ ë©”íŠ¸ë¦­
            'total_dates': 0,
            'total_stocks': 0,
            'total_factors': 0,
            'total_trades': 0,

            # íƒ€ì„ìŠ¤íƒ¬í”„
            'start_time': None,
            'end_time': None
        }

        # ì„ì‹œ ì¸¡ì •ìš© ë³€ìˆ˜
        self._timers = {}
        self._cpu_samples = []
        self.process = psutil.Process()

    def start(self):
        """ì „ì²´ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.metrics['start_time'] = datetime.now().isoformat()
        self.metrics['memory_start'] = self.process.memory_info().rss / (1024 ** 3)  # GB
        self._timers['total'] = time.time()
        logger.info("ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")

    def stop(self):
        """ì „ì²´ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        if 'total' in self._timers:
            self.metrics['total_time'] = time.time() - self._timers['total']
        self.metrics['end_time'] = datetime.now().isoformat()
        self.metrics['memory_current'] = self.process.memory_info().rss / (1024 ** 3)

        # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
        total_cache_access = self.metrics['cache_hits'] + self.metrics['cache_misses']
        if total_cache_access > 0:
            self.metrics['cache_hit_rate'] = self.metrics['cache_hits'] / total_cache_access

        # CPU í‰ê·  ê³„ì‚°
        if self._cpu_samples:
            self.metrics['cpu_average'] = sum(self._cpu_samples) / len(self._cpu_samples)

        logger.info("ğŸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")

    def start_timer(self, name: str):
        """íŠ¹ì • ì‘ì—…ì˜ ì‹œê°„ ì¸¡ì • ì‹œì‘"""
        self._timers[name] = time.time()
        logger.debug(f"â±ï¸ {name} ì‹œì‘")

    def stop_timer(self, name: str) -> float:
        """íŠ¹ì • ì‘ì—…ì˜ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ"""
        if name not in self._timers:
            return 0.0

        elapsed = time.time() - self._timers[name]

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metric_key = f"{name}_time"
        if metric_key in self.metrics:
            self.metrics[metric_key] = elapsed

        del self._timers[name]
        logger.debug(f"â±ï¸ {name} ì¢…ë£Œ: {elapsed:.2f}ì´ˆ")
        return elapsed

    def record_cache_hit(self):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        self.metrics['cache_hits'] += 1

    def record_cache_miss(self):
        """ìºì‹œ ë¯¸ìŠ¤ ê¸°ë¡"""
        self.metrics['cache_misses'] += 1

    def record_db_query(self, query_time: float = 0.0):
        """DB ì¿¼ë¦¬ ê¸°ë¡"""
        self.metrics['db_queries'] += 1
        self.metrics['db_query_time'] += query_time

    def update_memory_usage(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        current_memory = self.process.memory_info().rss / (1024 ** 3)  # GB
        self.metrics['memory_current'] = current_memory
        self.metrics['memory_peak'] = max(self.metrics['memory_peak'], current_memory)

    def update_cpu_usage(self):
        """í˜„ì¬ CPU ì‚¬ìš©ë¥  ì—…ë°ì´íŠ¸"""
        try:
            cpu_percent = self.process.cpu_percent(interval=0.1)
            self._cpu_samples.append(cpu_percent)
            self.metrics['cpu_peak'] = max(self.metrics['cpu_peak'], cpu_percent)
        except Exception as e:
            logger.debug(f"CPU ì¸¡ì • ì‹¤íŒ¨: {e}")

    def set_data_volume(self, total_dates: int = 0, total_stocks: int = 0,
                       total_factors: int = 0, total_trades: int = 0):
        """ë°ì´í„° ë³¼ë¥¨ ë©”íŠ¸ë¦­ ì„¤ì •"""
        if total_dates:
            self.metrics['total_dates'] = total_dates
        if total_stocks:
            self.metrics['total_stocks'] = total_stocks
        if total_factors:
            self.metrics['total_factors'] = total_factors
        if total_trades:
            self.metrics['total_trades'] = total_trades

    def get_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.metrics.copy()

    def log_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë¡œê·¸ë¡œ ì¶œë ¥"""
        self.stop()  # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°

        # í¬ë§·íŒ…ëœ ì¶œë ¥
        log_message = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¦¬í¬íŠ¸                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ â±ï¸  ì‹¤í–‰ ì‹œê°„ ë¶„ì„                                                     â•‘
â•‘   â€¢ ë°ì´í„° ë¡œë“œ: {self.metrics['data_load_time']:>10.2f}ì´ˆ                    â•‘
â•‘   â€¢ íŒ©í„° ê³„ì‚°:   {self.metrics['factor_calc_time']:>10.2f}ì´ˆ                   â•‘
â•‘   â€¢ ì‹œë®¬ë ˆì´ì…˜:  {self.metrics['simulation_time']:>10.2f}ì´ˆ                    â•‘
â•‘   â€¢ ê²°ê³¼ ì €ì¥:   {self.metrics['save_time']:>10.2f}ì´ˆ                         â•‘
â•‘   â€¢ ì „ì²´ ì‹œê°„:   {self.metrics['total_time']:>10.2f}ì´ˆ                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’¾ ìºì‹œ ì„±ëŠ¥                                                          â•‘
â•‘   â€¢ ìºì‹œ íˆíŠ¸:   {self.metrics['cache_hits']:>10,}íšŒ                          â•‘
â•‘   â€¢ ìºì‹œ ë¯¸ìŠ¤:   {self.metrics['cache_misses']:>10,}íšŒ                        â•‘
â•‘   â€¢ íˆíŠ¸ìœ¨:      {self.metrics['cache_hit_rate']:>10.1%}                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ—„ï¸  DB ì„±ëŠ¥                                                           â•‘
â•‘   â€¢ ì¿¼ë¦¬ ìˆ˜:     {self.metrics['db_queries']:>10,}íšŒ                          â•‘
â•‘   â€¢ ì¿¼ë¦¬ ì‹œê°„:   {self.metrics['db_query_time']:>10.2f}ì´ˆ                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’» ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰                                                       â•‘
â•‘   â€¢ ì‹œì‘ ë©”ëª¨ë¦¬: {self.metrics['memory_start']:>10.2f} GB                     â•‘
â•‘   â€¢ ìµœëŒ€ ë©”ëª¨ë¦¬: {self.metrics['memory_peak']:>10.2f} GB                      â•‘
â•‘   â€¢ í˜„ì¬ ë©”ëª¨ë¦¬: {self.metrics['memory_current']:>10.2f} GB                   â•‘
â•‘   â€¢ ìµœëŒ€ CPU:    {self.metrics['cpu_peak']:>10.1f}%                          â•‘
â•‘   â€¢ í‰ê·  CPU:    {self.metrics['cpu_average']:>10.1f}%                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“Š ë°ì´í„° ë³¼ë¥¨                                                        â•‘
â•‘   â€¢ ê±°ë˜ì¼ ìˆ˜:   {self.metrics['total_dates']:>10,}ì¼                         â•‘
â•‘   â€¢ ì¢…ëª© ìˆ˜:     {self.metrics['total_stocks']:>10,}ê°œ                        â•‘
â•‘   â€¢ íŒ©í„° ìˆ˜:     {self.metrics['total_factors']:>10,}ê°œ                       â•‘
â•‘   â€¢ ê±°ë˜ ìˆ˜:     {self.metrics['total_trades']:>10,}ê±´                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        logger.info(log_message)

        # JSON í˜•ì‹ìœ¼ë¡œë„ ë¡œê¹… (íŒŒì‹± ê°€ëŠ¥í•œ í˜•íƒœ)
        logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ JSON: {json.dumps(self.metrics, indent=2, default=str)}")

    def export_metrics(self, filepath: str):
        """ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, default=str, ensure_ascii=False)
        logger.info(f"ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥ë¨: {filepath}")

    @staticmethod
    def compare_metrics(before: Dict[str, Any], after: Dict[str, Any]) -> Dict[str, Any]:
        """ë‘ ë©”íŠ¸ë¦­ ì„¸íŠ¸ ë¹„êµ"""
        comparison = {}

        # ì‹œê°„ ë¹„êµ
        time_keys = ['data_load_time', 'factor_calc_time', 'simulation_time', 'save_time', 'total_time']
        for key in time_keys:
            if key in before and key in after:
                improvement = (before[key] - after[key]) / before[key] * 100 if before[key] > 0 else 0
                comparison[key] = {
                    'before': before[key],
                    'after': after[key],
                    'improvement_percent': improvement
                }

        # ìºì‹œ íˆíŠ¸ìœ¨ ë¹„êµ
        if 'cache_hit_rate' in before and 'cache_hit_rate' in after:
            comparison['cache_hit_rate'] = {
                'before': before['cache_hit_rate'],
                'after': after['cache_hit_rate'],
                'improvement': after['cache_hit_rate'] - before['cache_hit_rate']
            }

        # DB ì¿¼ë¦¬ ë¹„êµ
        if 'db_queries' in before and 'db_queries' in after:
            reduction = (before['db_queries'] - after['db_queries']) / before['db_queries'] * 100 if before['db_queries'] > 0 else 0
            comparison['db_queries'] = {
                'before': before['db_queries'],
                'after': after['db_queries'],
                'reduction_percent': reduction
            }

        # ë©”ëª¨ë¦¬ ë¹„êµ
        if 'memory_peak' in before and 'memory_peak' in after:
            reduction = (before['memory_peak'] - after['memory_peak']) / before['memory_peak'] * 100 if before['memory_peak'] > 0 else 0
            comparison['memory_peak'] = {
                'before': before['memory_peak'],
                'after': after['memory_peak'],
                'reduction_percent': reduction
            }

        return comparison


# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì  ì‚¬ìš©)
global_monitor = PerformanceMonitor()