# ğŸ”¥ ë°±í…ŒìŠ¤íŠ¸ Heavy Workload ë¶„ì„

## ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ì˜ ì‹¤ì œ ë¶€í•˜ ë¶„ì„

### ë°±í…ŒìŠ¤íŠ¸ 1íšŒ ì‹¤í–‰ ì‹œ ì²˜ë¦¬ëŸ‰

```python
# ì˜ˆì‹œ: 2023-01-01 ~ 2023-12-31 (1ë…„)
ì¢…ëª© ìˆ˜: 2,000ê°œ (KOSPI + KOSDAQ)
ê±°ë˜ì¼: 250ì¼
íŒ©í„°: 54ê°œ

ì´ ë°ì´í„° í¬ì¸íŠ¸: 2,000 Ã— 250 Ã— 54 = 27,000,000ê°œ
ë©”ëª¨ë¦¬ ì‚¬ìš©: ~2-3GB per backtest
CPU ì‹œê°„: 2-5ë¶„ (ìµœì í™” ì•ˆëœ ê²½ìš° 10-20ë¶„)
```

### ë¬¸ì œì 
1. **ë©”ëª¨ë¦¬**: í•œ ë²ˆì— 3GB Ã— 16ê°œ = **48GB í•„ìš”** ğŸ˜±
2. **CPU**: ëª¨ë“  ì½”ì–´ ì ìœ 
3. **DB ë¶€í•˜**: 2700ë§Œ row ì½ê¸°
4. **ë„¤íŠ¸ì›Œí¬**: ë°ì´í„° ì „ì†¡ ë³‘ëª©

## ğŸš¨ Celeryë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œ ì´ìœ 

### ë¬¸ì œ 1: ë©”ëª¨ë¦¬ í­ë°œ
```python
# Worker 16ê°œê°€ ë™ì‹œ ì‹¤í–‰í•˜ë©´
Worker 1: 3GB
Worker 2: 3GB
...
Worker 16: 3GB
ì´ ë©”ëª¨ë¦¬: 48GB ğŸ˜±

# ì„œë²„ í•œ ëŒ€ë¡œëŠ” ë¶ˆê°€ëŠ¥!
```

### ë¬¸ì œ 2: DB ë³‘ëª©
```python
# 16ê°œ ë°±í…ŒìŠ¤íŠ¸ê°€ ë™ì‹œì— DB ì¿¼ë¦¬
SELECT * FROM stock_prices
WHERE date BETWEEN '2023-01-01' AND '2023-12-31'
-- 2,000ê°œ ì¢…ëª© Ã— 250ì¼ = 500,000 rows

# 16ë°° = 8,000,000 rows ë™ì‹œ ì½ê¸°
# PostgreSQL ì—°ê²° ê³ ê°ˆ, Disk I/O ë³‘ëª©
```

### ë¬¸ì œ 3: ë°ì´í„° ì¤‘ë³µ ë¡œë”©
```python
# ê° Workerê°€ ê°™ì€ ë°ì´í„° ë¡œë“œ
Worker 1: 2023ë…„ ê°€ê²© ë°ì´í„° ë¡œë“œ (500,000 rows)
Worker 2: 2023ë…„ ê°€ê²© ë°ì´í„° ë¡œë“œ (500,000 rows)
Worker 3: 2023ë…„ ê°€ê²© ë°ì´í„° ë¡œë“œ (500,000 rows)
...
# ì—„ì²­ë‚œ ì¤‘ë³µê³¼ ë‚­ë¹„! ğŸ˜±
```

## âœ… ì˜¬ë°”ë¥¸ í•´ê²°ì±…

### 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë¶„ë¦¬ (ê°€ì¥ ì¤‘ìš”!)

```python
# ì˜ëª»ëœ ë°©ì‹ (í˜„ì¬)
async def run_backtest():
    # ë§¤ë²ˆ ëª¨ë“  ë°ì´í„° ë¡œë“œ
    price_data = await load_price_data()      # 500K rows
    financial_data = await load_financial()   # 100K rows
    factor_data = await calculate_factors()   # 27M ê³„ì‚°

    result = simulate_portfolio(...)  # ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸


# ì˜¬ë°”ë¥¸ ë°©ì‹ (ê°œì„ )
# Step 1: ë°ì´í„° ì‚¬ì „ ì¤€ë¹„ (1íšŒë§Œ)
async def prepare_backtest_data(date_range):
    """ëª¨ë“  ë°±í…ŒìŠ¤íŠ¸ê°€ ê³µìœ í•  ë°ì´í„° ì¤€ë¹„"""

    # Redisì— ìºì‹±
    key = f"prepared_data:{date_range}"

    if not redis.exists(key):
        # í•œ ë²ˆë§Œ ê³„ì‚°
        price_data = await load_price_data()
        factor_data = await calculate_all_factors()

        # ì••ì¶•í•´ì„œ Redisì— ì €ì¥
        compressed = compress(pickle.dumps({
            'prices': price_data,
            'factors': factor_data
        }))
        redis.set(key, compressed, ex=3600)  # 1ì‹œê°„ ìºì‹œ

    return key

# Step 2: ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë¹ ë¦„!)
async def run_backtest(data_key, conditions):
    """ì¤€ë¹„ëœ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰"""

    # Redisì—ì„œ ê°€ì ¸ì˜¤ê¸° (ë¹ ë¦„!)
    data = decompress(redis.get(data_key))

    # ì¡°ê±´ì— ë§ëŠ” ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰
    result = simulate_portfolio(data, conditions)

    return result
```

### 2ë‹¨ê³„: ë¶„ì‚° ìºì‹± ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI (API)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Preparation Service                â”‚
â”‚   (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„ - 1ì‹œê°„ë§ˆë‹¤)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Redis Cluster (Shared Cache)              â”‚
â”‚  - ê°€ê²© ë°ì´í„° (ì••ì¶•)                               â”‚
â”‚  - íŒ©í„° ë°ì´í„° (ì••ì¶•)                               â”‚
â”‚  - ì¬ë¬´ì œí‘œ ë°ì´í„°                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Worker 1â”‚ â”‚Worker 2â”‚ â”‚Worker Nâ”‚
â”‚(ê²½ëŸ‰í™”)â”‚ â”‚(ê²½ëŸ‰í™”)â”‚ â”‚(ê²½ëŸ‰í™”)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚          â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚PostgreSQLâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ìµœì í™”

```python
# app/services/backtest_optimized.py

class OptimizedBacktestEngine:
    """ìµœì í™”ëœ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.data_cache = {}

    async def run_backtest_optimized(
        self,
        backtest_id: UUID,
        conditions: dict,
        date_range: tuple
    ):
        """ìµœì í™”ëœ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        # 1. ìºì‹œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹ ë¦„!)
        cache_key = self._get_cache_key(date_range)
        cached_data = await self._get_cached_data(cache_key)

        if not cached_data:
            # ìºì‹œ ë¯¸ìŠ¤ - ë°ì´í„° ì¤€ë¹„
            cached_data = await self._prepare_and_cache_data(
                date_range, cache_key
            )

        # 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œë®¬ë ˆì´ì…˜
        result = await self._run_lightweight_simulation(
            cached_data, conditions
        )

        return result

    async def _get_cached_data(self, key: str):
        """Redisì—ì„œ ì••ì¶•ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""

        compressed = self.redis.get(key)
        if not compressed:
            return None

        # ì••ì¶• í•´ì œ (ë¹ ë¦„!)
        data = pickle.loads(zlib.decompress(compressed))
        return data

    async def _prepare_and_cache_data(self, date_range, key):
        """ë°ì´í„° ì¤€ë¹„ ë° ìºì‹±"""

        start, end = date_range

        # DBì—ì„œ ë°ì´í„° ë¡œë“œ (1íšŒë§Œ!)
        prices = await self.db.execute(
            select(StockPrice)
            .where(StockPrice.date.between(start, end))
        )

        # Pandasë¡œ ë³€í™˜
        price_df = pd.DataFrame([
            {
                'date': p.date,
                'stock_code': p.stock_code,
                'close': p.close_price,
                'volume': p.volume
            }
            for p in prices
        ])

        # íŒ©í„° ê³„ì‚° (ë²¡í„°í™”!)
        factor_df = self._calculate_factors_vectorized(price_df)

        # ì••ì¶•í•´ì„œ ìºì‹±
        data = {
            'prices': price_df.to_dict('records'),
            'factors': factor_df.to_dict('records')
        }

        compressed = zlib.compress(pickle.dumps(data))
        self.redis.setex(key, 3600, compressed)  # 1ì‹œê°„

        return data

    def _calculate_factors_vectorized(self, df: pd.DataFrame):
        """ë²¡í„°í™”ëœ íŒ©í„° ê³„ì‚° (ë¹ ë¦„!)"""

        # í•œ ë²ˆì— ëª¨ë“  íŒ©í„° ê³„ì‚°
        df['PER'] = df['price'] / df['eps']
        df['PBR'] = df['price'] / df['book_value']
        df['MOMENTUM_3M'] = df.groupby('stock_code')['price'].pct_change(60)
        # ... 54ê°œ íŒ©í„°

        return df

    async def _run_lightweight_simulation(self, data, conditions):
        """ê²½ëŸ‰í™”ëœ ì‹œë®¬ë ˆì´ì…˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""

        # DataFrame ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        prices_df = pd.DataFrame(data['prices'])
        factors_df = pd.DataFrame(data['factors'])

        # Generatorë¡œ ì¼ë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½!)
        holdings = {}
        cash = 100_000_000

        for date in prices_df['date'].unique():
            daily_prices = prices_df[prices_df['date'] == date]
            daily_factors = factors_df[factors_df['date'] == date]

            # ë§¤ìˆ˜/ë§¤ë„ ë¡œì§
            holdings, cash = self._process_trading_day(
                date, daily_prices, daily_factors,
                holdings, cash, conditions
            )

        return self._calculate_statistics(holdings, cash)
```

### 4ë‹¨ê³„: Celery + ì „ì²˜ë¦¬ ì¡°í•©

```python
# app/tasks/backtest_tasks.py

@celery_app.task(bind=True)
def prepare_data_background(self, date_range):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„"""

    # ì¸ê¸°ìˆëŠ” ê¸°ê°„ëŒ€ ë°ì´í„° ë¯¸ë¦¬ ìºì‹±
    common_ranges = [
        ('2023-01-01', '2023-12-31'),  # 1ë…„
        ('2022-01-01', '2023-12-31'),  # 2ë…„
        ('2020-01-01', '2023-12-31'),  # 3ë…„
    ]

    for start, end in common_ranges:
        engine = OptimizedBacktestEngine(redis_client)
        await engine._prepare_and_cache_data((start, end), f"data:{start}:{end}")


@celery_app.task(bind=True)
def run_backtest_task(self, backtest_id, config):
    """ìµœì í™”ëœ ë°±í…ŒìŠ¤íŠ¸ Task"""

    engine = OptimizedBacktestEngine(redis_client)

    # ìºì‹œëœ ë°ì´í„° ì‚¬ìš© (ë¹ ë¦„!)
    result = await engine.run_backtest_optimized(
        backtest_id=backtest_id,
        conditions=config['conditions'],
        date_range=(config['start_date'], config['end_date'])
    )

    return result
```

### 5ë‹¨ê³„: ë©”ëª¨ë¦¬ ìµœì í™”

```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

# Before (ìµœì í™” ì „)
ë©”ëª¨ë¦¬: 3GB per backtest
16ê°œ ë™ì‹œ: 48GB

# After (ìµœì í™” í›„)
ê³µìœ  ë°ì´í„° (Redis): 5GB (ì••ì¶•, ëª¨ë“  Worker ê³µìœ )
Worker ë©”ëª¨ë¦¬: 500MB per backtest
16ê°œ ë™ì‹œ: 8GB + 5GB = 13GB âœ…
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### ìµœì í™” ì „
```python
# ê° ë°±í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ë°ì´í„° ë¡œë“œ
DB ì¿¼ë¦¬: 500K rows Ã— 16 = 8M rows
ë©”ëª¨ë¦¬: 48GB
ì‹¤í–‰ ì‹œê°„: 5ë¶„/ë°±í…ŒìŠ¤íŠ¸
```

### ìµœì í™” í›„
```python
# ë°ì´í„° 1íšŒ ì¤€ë¹„, ì¬ì‚¬ìš©
DB ì¿¼ë¦¬: 500K rows Ã— 1 = 500K rows (16ë°° ê°ì†Œ!)
ë©”ëª¨ë¦¬: 13GB (3.7ë°° ê°ì†Œ!)
ì‹¤í–‰ ì‹œê°„: 30ì´ˆ/ë°±í…ŒìŠ¤íŠ¸ (10ë°° ë¹ ë¦„!)
```

## ğŸ¯ ê²°ë¡ : Kafka vs Celery (ì¬í‰ê°€)

### 100ëª… ë™ì‹œ ë°±í…ŒìŠ¤íŠ¸

#### âŒ Kafka ì—¬ì „íˆ ë¶ˆí•„ìš”
- KafkaëŠ” **ë©”ì‹œì§€ í**ì¼ ë¿
- **ë©”ëª¨ë¦¬/CPU/ë°ì´í„° ë¬¸ì œëŠ” í•´ê²° ëª»í•¨**
- Kafka Consumerë„ ê²°êµ­ ê°™ì€ ë°ì´í„° ë¡œë“œ í•„ìš”

#### âœ… ì˜¬ë°”ë¥¸ í•´ê²°ì±…
1. **ë°ì´í„° ì „ì²˜ë¦¬ + Redis ìºì‹±** (ê°€ì¥ ì¤‘ìš”!)
2. **ë²¡í„°í™” ì—°ì‚°** (Pandas/NumPy)
3. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜**
4. **Celeryë¡œ ì‘ì—… ë¶„ì‚°**

### ìµœì¢… ì•„í‚¤í…ì²˜

```yaml
# docker-compose.yml
services:
  # Redis (ëŒ€ìš©ëŸ‰ ìºì‹œ)
  redis:
    image: redis:7
    command: redis-server --maxmemory 10gb --maxmemory-policy lru
    volumes:
      - redis_data:/data

  # Data Preparation (ë°±ê·¸ë¼ìš´ë“œ)
  data-prep:
    build: .
    command: celery -A app.tasks.data_prep worker
    environment:
      - CELERY_QUEUE=data-preparation

  # Backtest Workers
  worker:
    build: .
    command: celery -A app.tasks.backtest_tasks worker --concurrency=2
    environment:
      - CELERY_QUEUE=backtest
    deploy:
      replicas: 4  # 8ê°œ ë™ì‹œ ë°±í…ŒìŠ¤íŠ¸

  # API
  api:
    build: .
    ports:
      - "8000:8000"
```

### ì„±ëŠ¥ ì˜ˆìƒ
```
ë°ì´í„° ì¤€ë¹„ ì‹œê°„: 5ë¶„ (1íšŒë§Œ, ë°±ê·¸ë¼ìš´ë“œ)
ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰: 30ì´ˆ/ê°œ
ë™ì‹œ ì²˜ë¦¬: 8ê°œ

100ëª… ìš”ì²­ â†’ ì•½ 6ë¶„ ë‚´ ëª¨ë‘ ì™„ë£Œ
(100 Ã· 8 = 12.5 ë°°ì¹˜ Ã— 30ì´ˆ = 6.25ë¶„)

ì¶©ë¶„íˆ ë¹ ë¦„! âœ…
```

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### Kafkaê°€ í•´ê²°í•˜ëŠ” ê²ƒ:
- âœ… ë©”ì‹œì§€ íì‰
- âœ… ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
- âœ… ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥

### Kafkaê°€ í•´ê²° ëª»í•˜ëŠ” ê²ƒ:
- âŒ ë°ì´í„° ì¤‘ë³µ ë¡œë”©
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- âŒ CPU ë³‘ëª©
- âŒ DB ë¶€í•˜

### ì§„ì§œ í•„ìš”í•œ ê²ƒ:
- âœ… **ë°ì´í„° ìºì‹±** (Redis)
- âœ… **ì „ì²˜ë¦¬ ë¶„ë¦¬**
- âœ… **ë²¡í„°í™” ì—°ì‚°**
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨ ì•Œê³ ë¦¬ì¦˜**
- âœ… Celery (ì‘ì—… ë¶„ì‚°)

## ğŸš€ ìµœì¢… ë‹µë³€

**100ëª… ìˆ˜ì¤€ì—ì„œëŠ” Kafka ë¶ˆí•„ìš”!**

**ëŒ€ì‹  ì´ë ‡ê²Œ í•˜ì„¸ìš”:**
1. Redisë¡œ ë°ì´í„° ìºì‹± (ê°€ì¥ ì¤‘ìš”!)
2. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„
3. ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ìµœì í™”
4. Celeryë¡œ ì‘ì—… ë¶„ì‚°

**ì´ë ‡ê²Œ í•˜ë©´:**
- ë©”ëª¨ë¦¬: 48GB â†’ 13GB
- ì†ë„: 5ë¶„ â†’ 30ì´ˆ
- DB ë¶€í•˜: 16ë°° ê°ì†Œ
- ë¹„ìš©: ì €ë ´

**KafkaëŠ” 1000ëª…+ ë˜ê³ , ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ í•„ìš”í•  ë•Œ!**