# ğŸš€ ë°±í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ í™•ì¥ì„± ê°€ì´ë“œ

## ğŸ“Š ë™ì‹œì„± ìš”êµ¬ì‚¬í•­ ë¶„ì„

### í˜„ì¬ ìš”êµ¬ì‚¬í•­
- **ë™ì‹œ ì ‘ì†**: 100ëª…
- **ë™ì‹œ ë°±í…ŒìŠ¤íŠ¸**: 100ê°œ (ìµœì•…ì˜ ê²½ìš°)
- **ë°±í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„**: 2-5ë¶„ (ì˜ˆìƒ)
- **ë°ì´í„° í¬ê¸°**: ì¤‘ê°„ ê·œëª¨

## ğŸ¯ Kafkaê°€ í•„ìš”í•œê°€?

### âŒ **100ëª… ìˆ˜ì¤€ì—ì„œëŠ” Kafka ë¶ˆí•„ìš”**

#### ì´ìœ :
1. **ì˜¤ë²„ì—”ì§€ë‹ˆì–´ë§**: KafkaëŠ” ìˆ˜ì²œ~ìˆ˜ë§Œ TPSë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë„êµ¬
2. **ë³µì¡ë„ ì¦ê°€**: Kafka í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ë¹„ìš© > ì–»ëŠ” ì´ë“
3. **ë¹„ìš©**: Kafka ì¸í”„ë¼ ì¶”ê°€ ë¹„ìš©
4. **í•™ìŠµ ê³¡ì„ **: Kafka ìš´ì˜ ì „ë¬¸ì„± í•„ìš”

### âœ… **ëŒ€ì‹  ì´ë ‡ê²Œ í•˜ì„¸ìš”**

## 1ë‹¨ê³„: Celery + Redis (ì¶”ì²œ) ğŸ‘

### ì•„í‚¤í…ì²˜
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FastAPI   â”‚
                    â”‚   (API)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Redis    â”‚
                    â”‚   (Queue)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker 1â”‚        â”‚ Worker 2â”‚      â”‚ Worker Nâ”‚
   â”‚(Celery) â”‚        â”‚(Celery) â”‚      â”‚(Celery) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ PostgreSQL  â”‚
                    â”‚   (ê²°ê³¼)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì¥ì :
- âœ… **ê°„ë‹¨**: ë¹ ë¥¸ êµ¬í˜„ (1-2ì¼)
- âœ… **ê²€ì¦ë¨**: ì—…ê³„ í‘œì¤€
- âœ… **í™•ì¥ ê°€ëŠ¥**: Worker ì¶”ê°€ë¡œ ìˆ˜í‰ í™•ì¥
- âœ… **ëª¨ë‹ˆí„°ë§**: Flowerë¡œ ì‰¬ìš´ ëª¨ë‹ˆí„°ë§
- âœ… **ë¹„ìš© íš¨ìœ¨ì **: Redisë§Œ ì¶”ê°€

### êµ¬í˜„ ì˜ˆì‹œ

#### 1. Celery ì„¤ì •
```python
# app/core/celery_app.py
from celery import Celery

celery_app = Celery(
    'backtest',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Seoul',
    enable_utc=True,

    # ë™ì‹œì„± ì„¤ì •
    worker_concurrency=4,  # Workerë‹¹ 4ê°œ ì‘ì—… ë™ì‹œ ì²˜ë¦¬
    worker_prefetch_multiplier=1,

    # íƒ€ì„ì•„ì›ƒ
    task_time_limit=600,  # 10ë¶„
    task_soft_time_limit=540,  # 9ë¶„
)
```

#### 2. ë°±í…ŒìŠ¤íŠ¸ Task
```python
# app/tasks/backtest_tasks.py
from app.core.celery_app import celery_app
from app.services.backtest import BacktestEngineGenPort

@celery_app.task(bind=True)
def run_backtest_async(self, backtest_id: str, config: dict):
    """ë¹„ë™ê¸° ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
    self.update_state(
        state='PROGRESS',
        meta={'current': 0, 'total': 100, 'status': 'Starting...'}
    )

    try:
        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        engine = BacktestEngineGenPort(db)
        result = await engine.run_backtest(**config)

        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        self.update_state(
            state='PROGRESS',
            meta={'current': 100, 'total': 100, 'status': 'Completed'}
        )

        return {
            'status': 'COMPLETED',
            'backtest_id': backtest_id,
            'result': result.dict()
        }

    except Exception as e:
        self.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise
```

#### 3. API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
```python
# app/api/v1/backtest.py
from app.tasks.backtest_tasks import run_backtest_async

@router.post("/backtest")
async def create_backtest(request: BacktestCreateRequest):
    """ë°±í…ŒìŠ¤íŠ¸ ìƒì„± ë° ë¹„ë™ê¸° ì‹¤í–‰"""

    backtest_id = uuid4()

    # Celery Taskë¡œ ë¹„ë™ê¸° ì‹¤í–‰
    task = run_backtest_async.delay(
        backtest_id=str(backtest_id),
        config=request.dict()
    )

    # Task ID ì €ì¥ (ì§„í–‰ìƒí™© ì¡°íšŒìš©)
    await save_task_mapping(backtest_id, task.id)

    return {
        "backtest_id": backtest_id,
        "task_id": task.id,
        "status": "QUEUED",
        "message": "Backtest queued for processing"
    }

@router.get("/backtest/{backtest_id}/status")
async def get_backtest_status(backtest_id: str):
    """ë°±í…ŒìŠ¤íŠ¸ ì§„í–‰ìƒí™© ì¡°íšŒ"""

    task_id = await get_task_id(backtest_id)
    task = celery_app.AsyncResult(task_id)

    if task.state == 'PENDING':
        return {"status": "QUEUED", "progress": 0}
    elif task.state == 'PROGRESS':
        return {
            "status": "RUNNING",
            "progress": task.info.get('current', 0),
            "message": task.info.get('status', '')
        }
    elif task.state == 'SUCCESS':
        return {"status": "COMPLETED", "progress": 100}
    elif task.state == 'FAILURE':
        return {"status": "FAILED", "error": str(task.info)}
```

#### 4. Worker ì‹¤í–‰
```bash
# Worker ì‹œì‘ (4ê°œ í”„ë¡œì„¸ìŠ¤)
celery -A app.core.celery_app worker \
    --loglevel=info \
    --concurrency=4 \
    --pool=prefork

# Flower ëª¨ë‹ˆí„°ë§ (ì„ íƒ)
celery -A app.core.celery_app flower --port=5555
```

### ì„±ëŠ¥ ê³„ì‚°
```
Worker ìˆ˜: 4ê°œ
Workerë‹¹ ë™ì‹œ ì‘ì—…: 4ê°œ
ì´ ë™ì‹œ ì²˜ë¦¬: 16ê°œ ë°±í…ŒìŠ¤íŠ¸

ë°±í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„: 3ë¶„ í‰ê· 
ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: 16 * (60ë¶„ / 3ë¶„) = 320ê°œ/ì‹œê°„

100ëª…ì´ ë™ì‹œ ìš”ì²­ â†’ ëŒ€ê¸° ì‹œê°„: ìµœëŒ€ 18ë¶„
(100ëª… Ã· 16 = 6.25 ë°°ì¹˜ Ã— 3ë¶„ = 18.75ë¶„)
```

## 2ë‹¨ê³„: ë” ë¹ ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´?

### ì˜µì…˜ 1: Worker ì¦ì„¤
```bash
# EC2 ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€ (ìˆ˜í‰ í™•ì¥)
# ì„œë²„ 1: Worker 4ê°œ
# ì„œë²„ 2: Worker 4ê°œ
# ì„œë²„ 3: Worker 4ê°œ
# â†’ ì´ 48ê°œ ë™ì‹œ ë°±í…ŒìŠ¤íŠ¸
```

### ì˜µì…˜ 2: ìš°ì„ ìˆœìœ„ í
```python
# VIP ì‚¬ìš©ìëŠ” ìš°ì„  ì²˜ë¦¬
@celery_app.task(priority=9)  # ë†’ì€ ìš°ì„ ìˆœìœ„
def run_vip_backtest(backtest_id, config):
    pass

@celery_app.task(priority=1)  # ë‚®ì€ ìš°ì„ ìˆœìœ„
def run_normal_backtest(backtest_id, config):
    pass
```

### ì˜µì…˜ 3: ë°±í…ŒìŠ¤íŠ¸ ìµœì í™”
```python
# 1. íŒ©í„° ê³„ì‚° ìºì‹±
@lru_cache(maxsize=1000)
def calculate_factors(date, stock_code):
    pass

# 2. ë²¡í„°í™” ì—°ì‚°
df['PER'] = df['price'] / df['eps']  # í•œë²ˆì— ê³„ì‚°

# 3. ë³‘ë ¬ ì²˜ë¦¬
async with asyncio.TaskGroup() as tg:
    for date in dates:
        tg.create_task(process_date(date))
```

## 3ë‹¨ê³„: 1000ëª…+ ìˆ˜ì¤€ì´ë¼ë©´?

### ì´ì œ Kafka ê³ ë ¤ ì‹œì 

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   FastAPI    â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Kafka     â”‚
                â”‚  (ë©”ì‹œì§€í)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼              â–¼              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Consumerâ”‚     â”‚Consumerâ”‚    â”‚Consumerâ”‚
   â”‚Group 1 â”‚     â”‚Group 2 â”‚    â”‚Group 3 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ê²°ê³¼ ì €ì¥ì†Œ    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Kafka ì¥ì  (ëŒ€ê·œëª¨)
- âœ… ì´ˆë‹¹ ìˆ˜ë§Œ ë©”ì‹œì§€ ì²˜ë¦¬
- âœ… ë©”ì‹œì§€ ì˜êµ¬ ì €ì¥
- âœ… ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
- âœ… ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬

### Kafka ë‹¨ì 
- âŒ ë³µì¡í•œ ì„¤ì •
- âŒ ë†’ì€ ìš´ì˜ ë¹„ìš©
- âŒ Zookeeper í•„ìš” (Kafka 2.x)
- âŒ í•™ìŠµ ê³¡ì„ 

## 4ë‹¨ê³„: ìºì‹± ì „ëµ (í•„ìˆ˜!)

### Redis ìºì‹±
```python
# app/core/cache.py
import redis
import pickle

redis_client = redis.Redis(host='localhost', port=6379)

async def get_cached_factors(date: str):
    """íŒ©í„° ë°ì´í„° ìºì‹±"""
    key = f"factors:{date}"
    cached = redis_client.get(key)

    if cached:
        return pickle.loads(cached)

    # ê³„ì‚°
    factors = await calculate_all_factors(date)

    # ìºì‹œ ì €ì¥ (1ì‹œê°„)
    redis_client.setex(key, 3600, pickle.dumps(factors))

    return factors
```

## ğŸ“Š ë‹¨ê³„ë³„ ì „ëµ ìš”ì•½

### Phase 1: 100ëª… ìˆ˜ì¤€ (í˜„ì¬)
```
âœ… Celery + Redis
âœ… Worker 4-8ê°œ
âœ… ê¸°ë³¸ ìºì‹±
ì´ ë¹„ìš©: $50-100/ì›”
ê°œë°œ ê¸°ê°„: 2-3ì¼
```

### Phase 2: 500ëª… ìˆ˜ì¤€
```
âœ… Celery + Redis (í™•ì¥)
âœ… Worker 16-32ê°œ (ì—¬ëŸ¬ ì„œë²„)
âœ… Redis Cluster
âœ… ê³ ê¸‰ ìºì‹±
ì´ ë¹„ìš©: $200-300/ì›”
ê°œë°œ ê¸°ê°„: 1ì£¼
```

### Phase 3: 1000ëª…+ ìˆ˜ì¤€
```
âœ… Kafka + Consumer Groups
âœ… Auto-scaling Workers
âœ… Redis Cluster
âœ… CDN + Edge Caching
ì´ ë¹„ìš©: $500-1000/ì›”
ê°œë°œ ê¸°ê°„: 2-3ì£¼
```

## ğŸ¯ ì¶”ì²œ ì†”ë£¨ì…˜ (100ëª… ê¸°ì¤€)

### ìµœì†Œ êµ¬ì„± (MVP)
```yaml
# docker-compose.yml
version: '3.8'

services:
  # Redis (í + ìºì‹œ)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # FastAPI
  api:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - postgres

  # Celery Worker (4ê°œ)
  worker:
    build: .
    command: celery -A app.core.celery_app worker --concurrency=4
    depends_on:
      - redis
      - postgres
    deploy:
      replicas: 2  # 2ê°œ ì»¨í…Œì´ë„ˆ = 8ê°œ ë™ì‹œ ë°±í…ŒìŠ¤íŠ¸

  # Flower (ëª¨ë‹ˆí„°ë§)
  flower:
    build: .
    command: celery -A app.core.celery_app flower
    ports:
      - "5555:5555"
    depends_on:
      - redis

  # PostgreSQL
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: stocklab
      POSTGRES_PASSWORD: password

volumes:
  redis_data:
  postgres_data:
```

### ì‹¤í–‰
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose up -d

# Worker ìŠ¤ì¼€ì¼ë§ (í•„ìš”ì‹œ)
docker-compose up -d --scale worker=4  # 16ê°œ ë™ì‹œ ì²˜ë¦¬
```

## ğŸ’° ë¹„ìš© ë¹„êµ

### Celery + Redis êµ¬ì„±
```
- EC2 t3.medium (API): $30/ì›”
- EC2 t3.medium (Worker x2): $60/ì›”
- ElastiCache Redis: $15/ì›”
- RDS PostgreSQL: $25/ì›”
ì´: $130/ì›”
```

### Kafka êµ¬ì„±
```
- EC2 t3.medium (API): $30/ì›”
- EC2 t3.large (Kafka x3): $210/ì›”
- EC2 t3.medium (Worker x2): $60/ì›”
- ElastiCache Redis: $15/ì›”
- RDS PostgreSQL: $25/ì›”
ì´: $340/ì›” (2.6ë°° ë¹„ìŒˆ!)
```

## ğŸ ê²°ë¡ 

### 100ëª… ìˆ˜ì¤€ì—ì„œëŠ”:
1. âœ… **Celery + Redis** ì‚¬ìš©
2. âœ… Worker 2-4ê°œë¡œ ì‹œì‘
3. âœ… í•„ìš”ì‹œ Workerë§Œ ì¦ì„¤
4. âœ… ìºì‹± ìµœì í™”
5. âŒ **KafkaëŠ” ë¶ˆí•„ìš”**

### Kafkaë¥¼ ê³ ë ¤í•  ì‹œì :
- ë™ì‹œ ì‚¬ìš©ì 1000ëª…+
- ì´ˆë‹¹ 100+ ë°±í…ŒìŠ¤íŠ¸ ìš”ì²­
- ë³µì¡í•œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í•„ìš”
- ë©”ì‹œì§€ ì˜êµ¬ ì €ì¥ í•„ìš”
- ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

**ì§€ê¸ˆì€ Celeryë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤!** ğŸš€