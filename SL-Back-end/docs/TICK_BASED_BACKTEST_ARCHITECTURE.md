# âš¡ Tick ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ ì•„í‚¤í…ì²˜

## ğŸ”¥ ë°ì´í„° ê·œëª¨ ë¹„êµ

### ì¼ë´‰ ê¸°ë°˜ (í˜„ì¬)
```python
# 1ë…„ ë°±í…ŒìŠ¤íŠ¸
ì¢…ëª©: 2,000ê°œ
ê±°ë˜ì¼: 250ì¼
ë°ì´í„° í¬ì¸íŠ¸: 2,000 Ã— 250 = 500,000ê°œ
ë°ì´í„° í¬ê¸°: ~100MB
ì²˜ë¦¬ ì‹œê°„: 30ì´ˆ - 5ë¶„
```

### Tick ê¸°ë°˜ (ì²´ê²° ë‹¨ìœ„)
```python
# 1ì¼ ë°±í…ŒìŠ¤íŠ¸ë§Œ í•´ë„...
ì¢…ëª©: 2,000ê°œ
ê±°ë˜ì‹œê°„: 6ì‹œê°„ (09:00-15:30)
í‰ê·  Tick: 100ê°œ/ì¢…ëª©/ë¶„
Tick ìˆ˜: 2,000 Ã— 360ë¶„ Ã— 100 = 72,000,000ê°œ/ì¼

# 1ë…„ì´ë©´
72M Ã— 250ì¼ = 18,000,000,000ê°œ (180ì–µ ê°œ!) ğŸ˜±
ë°ì´í„° í¬ê¸°: ~1-2TB
ì²˜ë¦¬ ì‹œê°„: ???
```

## ğŸš¨ Tick ê¸°ë°˜ì˜ ë„ì „ ê³¼ì œ

### 1. ë°ì´í„° ì €ì¥ ë¬¸ì œ
```python
# PostgreSQLë¡œëŠ” ë¶ˆê°€ëŠ¥!
18B rows Ã— í‰ê·  100 bytes = 1.8TB

# ì¿¼ë¦¬ í•œ ë²ˆì—
SELECT * FROM tick_data WHERE date = '2023-01-01'
â†’ 72M rows ì½ê¸° â†’ íƒ€ì„ì•„ì›ƒ!
```

### 2. ë©”ëª¨ë¦¬ í­ë°œ
```python
# í•˜ë£¨ì¹˜ ë°ì´í„°ë§Œ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ë©´
72M rows Ã— 100 bytes = 7.2GB (í•˜ë£¨ë§Œ!)

# 1ë…„ì¹˜ëŠ”?
250ì¼ Ã— 7.2GB = 1.8TB ğŸ˜±ğŸ˜±ğŸ˜±
ì„œë²„ ë©”ëª¨ë¦¬ ì´ˆê³¼!
```

### 3. ì²˜ë¦¬ ì‹œê°„
```python
# ì´ˆë‹¹ 100ë§Œê°œ ì²˜ë¦¬í•´ë„
18B Ã· 1M = 18,000ì´ˆ = 5ì‹œê°„!

# ì‹¤ì œë¡œëŠ” ë” ëŠë¦¼ (ì¡°ê±´ í‰ê°€, ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜ ë“±)
ì˜ˆìƒ ì‹œê°„: 10-20ì‹œê°„/ë°±í…ŒìŠ¤íŠ¸
```

## âœ… Tick ê¸°ë°˜ì´ë¼ë©´: Kafka + ì‹œê³„ì—´ DB í•„ìˆ˜!

### ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Tick Data Ingestion                     â”‚
â”‚   (ì‹¤ì‹œê°„ ì²´ê²° ë°ì´í„° ìˆ˜ì§‘ - Kafka Producer)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Kafka Cluster                       â”‚
â”‚   Topic: tick-data (Partitioned by Stock Code)      â”‚
â”‚   Retention: 7 days (ì••ì¶•)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼        â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Consumerâ”‚ â”‚Consumerâ”‚ â”‚Consumerâ”‚
â”‚Group 1 â”‚ â”‚Group 2 â”‚ â”‚Group 3 â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚          â”‚          â”‚
     â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      TimescaleDB / ClickHouse        â”‚
â”‚  (ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ - ì´ˆê³ ì†)     â”‚
â”‚  - Hypertables (ìë™ íŒŒí‹°ì…”ë‹)      â”‚
â”‚  - Columnar Storage (ì••ì¶•)          â”‚
â”‚  - Continuous Aggregates            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì™œ Kafkaê°€ í•„ìš”í•œê°€?

#### 1. ì‹¤ì‹œê°„ Tick ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
```python
# ê±°ë˜ì†Œì—ì„œ ì´ˆë‹¹ ìˆ˜ì‹­ë§Œ Tick ë°œìƒ
ì´ˆë‹¹ Tick: 300,000ê°œ
ë¶„ë‹¹ Tick: 18,000,000ê°œ
í•˜ë£¨ Tick: 72,000,000ê°œ

# Kafkaë§Œ ì´ ì†ë„ë¥¼ ê°ë‹¹ ê°€ëŠ¥
Kafka ì²˜ë¦¬ëŸ‰: ì´ˆë‹¹ ìˆ˜ë°±ë§Œ ë©”ì‹œì§€
Redis Queue: ì´ˆë‹¹ ìˆ˜ë§Œ ê°œ (ë¶€ì¡±!)
```

#### 2. íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
```python
# Kafka Topic Partitioning
Topic: tick-data
Partitions:
  - Partition 0: 005930 (ì‚¼ì„±ì „ì)
  - Partition 1: 000660 (SKí•˜ì´ë‹‰ìŠ¤)
  - Partition 2: 035420 (NAVER)
  ...

# Consumer Groupì´ ë³‘ë ¬ë¡œ ì²˜ë¦¬
Consumer 1 â†’ Partition 0-99
Consumer 2 â†’ Partition 100-199
Consumer 3 â†’ Partition 200-299
...
```

#### 3. Replay ê°€ëŠ¥ (ë°±í…ŒìŠ¤íŠ¸ í•µì‹¬!)
```python
# KafkaëŠ” ë©”ì‹œì§€ë¥¼ ë³´ê´€
# ë°±í…ŒìŠ¤íŠ¸ = ê³¼ê±° Tick ë°ì´í„° Replay

# 2023-01-01 00:00:00ë¶€í„° Replay
kafka_consumer = KafkaConsumer(
    'tick-data',
    auto_offset_reset='earliest',
    enable_auto_commit=False
)

# ì‹œê°„ ìˆœì„œëŒ€ë¡œ Tick ì²˜ë¦¬
for tick in kafka_consumer:
    process_tick(tick)
```

## ğŸ—ï¸ Tick ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ ìŠ¤íƒ

### Stack 1: ì‹œê³„ì—´ DB (í•„ìˆ˜!)

#### TimescaleDB (PostgreSQL í™•ì¥)
```sql
-- Hypertable ìƒì„± (ìë™ íŒŒí‹°ì…”ë‹)
CREATE TABLE tick_data (
    time TIMESTAMPTZ NOT NULL,
    stock_code VARCHAR(10),
    price DECIMAL(10, 2),
    volume INTEGER,
    bid_price DECIMAL(10, 2),
    ask_price DECIMAL(10, 2)
);

-- Hypertableë¡œ ë³€í™˜
SELECT create_hypertable('tick_data', 'time');

-- ìë™ ì••ì¶• (ì˜¤ë˜ëœ ë°ì´í„°)
ALTER TABLE tick_data SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'stock_code'
);

-- ë¹ ë¥¸ ì¿¼ë¦¬
SELECT * FROM tick_data
WHERE time BETWEEN '2023-01-01' AND '2023-01-02'
AND stock_code = '005930'
ORDER BY time
-- Partition pruningìœ¼ë¡œ ì´ˆê³ ì†!
```

**ì¥ì :**
- PostgreSQL í˜¸í™˜
- SQL ì‚¬ìš© ê°€ëŠ¥
- ìë™ íŒŒí‹°ì…”ë‹
- ì••ì¶• (10:1)

#### ClickHouse (ë” ë¹ ë¦„!)
```sql
-- í…Œì´ë¸” ìƒì„±
CREATE TABLE tick_data (
    time DateTime,
    stock_code String,
    price Decimal(10, 2),
    volume UInt32
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(time)
ORDER BY (stock_code, time);

-- ì´ˆê³ ì† ì¿¼ë¦¬
SELECT
    stock_code,
    avg(price),
    sum(volume)
FROM tick_data
WHERE time >= '2023-01-01'
GROUP BY stock_code
-- 180ì–µ rowsë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ì²˜ë¦¬!
```

**ì¥ì :**
- ì»¬ëŸ¼ ì €ì¥ (ì••ì¶• 50:1)
- ì´ˆê³ ì† (TimescaleDBë³´ë‹¤ 10ë°° ë¹ ë¦„)
- ìˆ˜í‰ í™•ì¥

**ë‹¨ì :**
- SQL ì•½ê°„ ë‹¤ë¦„
- íŠ¸ëœì­ì…˜ ì—†ìŒ

### Stack 2: Kafka + Flink (ì‹¤ì‹œê°„ ì²˜ë¦¬)

```python
# Apache Flink (ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬)
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer

env = StreamExecutionEnvironment.get_execution_environment()

# Kafkaì—ì„œ Tick ìŠ¤íŠ¸ë¦¼
kafka_consumer = FlinkKafkaConsumer(
    topics='tick-data',
    deserialization_schema=...,
    properties={'bootstrap.servers': 'localhost:9092'}
)

tick_stream = env.add_source(kafka_consumer)

# ì‹¤ì‹œê°„ ì§‘ê³„
tick_stream \
    .key_by(lambda x: x['stock_code']) \
    .window(TumblingProcessingTimeWindows.of(Time.seconds(1))) \
    .aggregate(TickAggregator()) \
    .add_sink(...)

env.execute()
```

### Stack 3: DuckDB (ë¶„ì„ìš©)

```python
# Parquet íŒŒì¼ë¡œ ì €ì¥ í›„ DuckDBë¡œ ë¶„ì„
import duckdb

# 1TB ë°ì´í„°ë„ ë¹ ë¥´ê²Œ ì¿¼ë¦¬
con = duckdb.connect()

result = con.execute("""
    SELECT
        stock_code,
        date_trunc('minute', time) as minute,
        avg(price) as avg_price,
        sum(volume) as total_volume
    FROM read_parquet('tick_data/*.parquet')
    WHERE time >= '2023-01-01'
    GROUP BY stock_code, minute
""").fetchdf()

# Pandas DataFrameìœ¼ë¡œ ë°±í…ŒìŠ¤íŠ¸
```

## ğŸ“Š Tick ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ

### PostgreSQL (í˜„ì¬)
```
ë°ì´í„°: 500K rows (ì¼ë´‰)
ì¿¼ë¦¬ ì‹œê°„: 0.1ì´ˆ
ë°±í…ŒìŠ¤íŠ¸: 5ë¶„
âœ… ì¼ë´‰ì—ëŠ” ì í•©
âŒ Tickì—ëŠ” ë¶ˆê°€ëŠ¥
```

### TimescaleDB + Kafka
```
ë°ì´í„°: 18B rows (Tick)
ì¿¼ë¦¬ ì‹œê°„: 1-5ì´ˆ (Hypertable)
ë°±í…ŒìŠ¤íŠ¸: 1-2ì‹œê°„
âœ… Tick ê°€ëŠ¥
âš ï¸ ë¹„ìš© ë†’ìŒ
```

### ClickHouse + Kafka + Flink
```
ë°ì´í„°: 18B rows (Tick)
ì¿¼ë¦¬ ì‹œê°„: 0.1-1ì´ˆ
ë°±í…ŒìŠ¤íŠ¸: 10-30ë¶„
âœ… Tick ìµœì 
âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬
âŒ ë³µì¡ë„ ë§¤ìš° ë†’ìŒ
```

## ğŸ’° ë¹„ìš© ë¹„êµ (Tick ê¸°ë°˜)

### ìµœì†Œ êµ¬ì„±
```
- ClickHouse (3 nodes): $300/ì›”
- Kafka (3 brokers): $200/ì›”
- Flink (2 workers): $150/ì›”
- S3 (ìŠ¤í† ë¦¬ì§€): $100/ì›”
- EC2 (API): $50/ì›”
ì´: $800-1000/ì›”
```

### í”„ë¡œë•ì…˜ êµ¬ì„±
```
- ClickHouse Cluster (9 nodes): $900/ì›”
- Kafka Cluster (5 brokers): $400/ì›”
- Flink Cluster (5 workers): $400/ì›”
- S3 (ìŠ¤í† ë¦¬ì§€): $300/ì›”
- Load Balancer: $50/ì›”
ì´: $2000-3000/ì›”
```

## ğŸ¯ Tick ë°±í…ŒìŠ¤íŠ¸ êµ¬í˜„ ì˜ˆì‹œ

### 1. Kafka Producer (Tick ìˆ˜ì§‘)
```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# ì‹¤ì‹œê°„ Tick ì „ì†¡
def send_tick(stock_code, price, volume, timestamp):
    tick = {
        'stock_code': stock_code,
        'price': price,
        'volume': volume,
        'timestamp': timestamp.isoformat()
    }
    producer.send('tick-data', value=tick, key=stock_code.encode())
```

### 2. Kafka Consumer (Tick ì €ì¥)
```python
from kafka import KafkaConsumer
import clickhouse_driver

consumer = KafkaConsumer(
    'tick-data',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=False
)

client = clickhouse_driver.Client('localhost')

# Batch insert (ì„±ëŠ¥ ìµœì í™”)
batch = []
for message in consumer:
    tick = json.loads(message.value)
    batch.append(tick)

    if len(batch) >= 10000:
        # Bulk insert
        client.execute(
            'INSERT INTO tick_data VALUES',
            batch
        )
        batch = []
        consumer.commit()
```

### 3. Tick ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
```python
class TickBacktestEngine:
    """Tick ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„"""

    def __init__(self, clickhouse_client):
        self.db = clickhouse_client

    async def run_tick_backtest(
        self,
        stock_code: str,
        start_time: datetime,
        end_time: datetime,
        strategy: callable
    ):
        """Tick ë‹¨ìœ„ ë°±í…ŒìŠ¤íŠ¸"""

        # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        query = f"""
        SELECT time, price, volume
        FROM tick_data
        WHERE stock_code = '{stock_code}'
        AND time BETWEEN '{start_time}' AND '{end_time}'
        ORDER BY time
        """

        # Generatorë¡œ Tick í•˜ë‚˜ì”© ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        tick_stream = self.db.execute_iter(query)

        position = None
        cash = 100_000_000
        trades = []

        for tick in tick_stream:
            time, price, volume = tick

            # ì „ëµ ì‹œê·¸ë„
            signal = strategy(tick, position)

            # ë§¤ìˆ˜
            if signal == 'BUY' and position is None:
                shares = cash // price
                position = {
                    'entry_time': time,
                    'entry_price': price,
                    'shares': shares
                }
                cash -= shares * price
                trades.append(('BUY', time, price, shares))

            # ë§¤ë„
            elif signal == 'SELL' and position:
                cash += position['shares'] * price
                trades.append(('SELL', time, price, position['shares']))
                position = None

        return {
            'trades': trades,
            'final_cash': cash,
            'final_position': position
        }
```

### 4. ì‹¤ì‹œê°„ ì „ëµ ì‹œê·¸ë„
```python
def momentum_strategy(tick, position):
    """Tick ê¸°ë°˜ ëª¨ë©˜í…€ ì „ëµ"""

    # ìµœê·¼ 100 Tickì˜ ì´ë™í‰ê· 
    recent_prices = get_recent_ticks(tick['stock_code'], 100)
    ma = sum(recent_prices) / len(recent_prices)

    # ë§¤ìˆ˜ ì‹œê·¸ë„
    if tick['price'] > ma * 1.02 and position is None:
        return 'BUY'

    # ë§¤ë„ ì‹œê·¸ë„
    if position and tick['price'] < position['entry_price'] * 0.98:
        return 'SELL'  # 2% ì†ì ˆ

    return 'HOLD'
```

## ğŸ“Š ê²°ë¡ 

### ì¼ë´‰ ê¸°ë°˜ (í˜„ì¬)
```
ë°ì´í„°: 500K rows
DB: PostgreSQL âœ…
Queue: Celery + Redis âœ…
ë¹„ìš©: $130/ì›” âœ…
```

### Tick ê¸°ë°˜
```
ë°ì´í„°: 18B rows
DB: ClickHouse / TimescaleDB í•„ìˆ˜! âœ…
Queue: Kafka í•„ìˆ˜! âœ…
Stream: Flink ê¶Œì¥ âœ…
ë¹„ìš©: $1000-3000/ì›” âš ï¸
```

## ğŸ¯ ìµœì¢… ë‹µë³€

### í˜„ì¬ í”„ë¡œì íŠ¸ (ì¼ë´‰)
- âŒ Kafka ë¶ˆí•„ìš”
- âœ… Celery + Redis + PostgreSQL

### Tick ê¸°ë°˜ìœ¼ë¡œ í™•ì¥ ì‹œ
- âœ… **Kafka í•„ìˆ˜!**
- âœ… **ClickHouse / TimescaleDB í•„ìˆ˜!**
- âœ… **Flink ê¶Œì¥**
- ğŸ’° **ë¹„ìš© 10ë°° ì¦ê°€**

**Tick ê¸°ë°˜ì€ ì™„ì „íˆ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤!** ğŸš€

**í˜„ì¬ëŠ” ì¼ë´‰ìœ¼ë¡œ ì‹œì‘í•˜ê³ , ìˆ˜ìµ ë°œìƒ í›„ Tickìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.**