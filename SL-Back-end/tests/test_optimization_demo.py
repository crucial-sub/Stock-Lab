#!/usr/bin/env python3
"""
ë°±í…ŒìŠ¤íŠ¸ ìµœì í™” ë°ëª¨
3ê°€ì§€ ìµœì í™” ê¸°ëŠ¥ì„ ì‹œì—°í•©ë‹ˆë‹¤:
1. ë³‘ë ¬ ì²˜ë¦¬
2. ì„ íƒì  íŒ©í„° ê³„ì‚°
3. Redis ìºì‹±
"""

import asyncio
import time
import logging
from datetime import date
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from app.core.cache import cache

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


async def demo_redis_caching():
    """Redis ìºì‹± ë°ëª¨"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¦ Redis ìºì‹± ë°ëª¨")
    logger.info("=" * 60)

    try:
        await cache.initialize()
        logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")

        # ìºì‹œ í‚¤ ìƒì„±
        test_key = cache._generate_key("test", {"date": "2024-12-01", "factor": "PER"})

        # 1. ì²« ë²ˆì§¸ í˜¸ì¶œ - ìºì‹œ ë¯¸ìŠ¤
        logger.info("\nì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤):")
        start = time.time()

        async def expensive_calculation():
            """ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜"""
            await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
            return {"PER": 15.5, "PBR": 1.2}

        result = await cache.get_or_set(test_key, expensive_calculation, ttl=60)
        elapsed = time.time() - start
        logger.info(f"  ê²°ê³¼: {result}")
        logger.info(f"  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")

        # 2. ë‘ ë²ˆì§¸ í˜¸ì¶œ - ìºì‹œ íˆíŠ¸
        logger.info("\në‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸):")
        start = time.time()
        result = await cache.get(test_key)
        elapsed = time.time() - start
        logger.info(f"  ê²°ê³¼: {result}")
        logger.info(f"  ì†Œìš”ì‹œê°„: {elapsed:.4f}ì´ˆ (1000ë°° ì´ìƒ ë¹ ë¦„!)")

        # ìºì‹œ í†µê³„
        stats = await cache.get_cache_stats()
        logger.info(f"\nğŸ“Š ìºì‹œ í†µê³„:")
        logger.info(f"  íˆíŠ¸ìœ¨: {stats.get('hit_ratio', 0):.1%}")
        logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats.get('used_memory_human', 'N/A')}")

    except Exception as e:
        logger.error(f"âŒ Redis ìºì‹± ì‹¤íŒ¨: {e}")


async def demo_selective_calculation():
    """ì„ íƒì  íŒ©í„° ê³„ì‚° ë°ëª¨"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ¯ ì„ íƒì  íŒ©í„° ê³„ì‚° ë°ëª¨")
    logger.info("=" * 60)

    from app.services.backtest import BacktestEngine

    # ê°„ë‹¨í•œ BacktestCondition í´ë˜ìŠ¤ ì •ì˜
    class BacktestCondition:
        def __init__(self, exp_left_side, inequality, exp_right_side):
            self.exp_left_side = exp_left_side
            self.inequality = inequality
            self.exp_right_side = exp_right_side

    # ë§¤ìˆ˜ ì¡°ê±´ ì„¤ì • (PBRë§Œ ì‚¬ìš©)
    buy_conditions = [
        BacktestCondition(
            exp_left_side="ê¸°ë³¸ê°’({PBR})",
            inequality="<",
            exp_right_side=5.0
        )
    ]

    # BacktestEngine ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (DB ì—†ì´)
    class MockDB:
        pass

    engine = BacktestEngine(MockDB())

    # í•„ìš”í•œ íŒ©í„° ì¶”ì¶œ
    required_factors = engine._extract_required_factors(buy_conditions, "PER")

    logger.info(f"ë§¤ìˆ˜ ì¡°ê±´: PBR < 5")
    logger.info(f"ìš°ì„ ìˆœìœ„: PER")
    logger.info(f"â¡ï¸  í•„ìš”í•œ íŒ©í„°ë§Œ ê³„ì‚°: {required_factors}")
    logger.info(f"    (ê¸°ì¡´: 12ê°œ íŒ©í„° â†’ ìµœì í™”: {len(required_factors)}ê°œ íŒ©í„°)")
    logger.info(f"    ì„±ëŠ¥ ê°œì„ : {(12-len(required_factors))/12*100:.0f}% ê³„ì‚°ëŸ‰ ê°ì†Œ")


async def demo_parallel_processing():
    """ë³‘ë ¬ ì²˜ë¦¬ ë°ëª¨"""
    logger.info("\n" + "=" * 60)
    logger.info("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ë°ëª¨")
    logger.info("=" * 60)

    # ì‹œë®¬ë ˆì´ì…˜: 100ê°œ ë‚ ì§œë¥¼ ì²˜ë¦¬
    dates = [date(2024, 1, 1) for _ in range(100)]

    async def process_date(d):
        """ë‚ ì§œë³„ íŒ©í„° ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜"""
        await asyncio.sleep(0.01)  # 10ms ì†Œìš”
        return f"processed_{d}"

    # 1. ìˆœì°¨ ì²˜ë¦¬
    logger.info("\nìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹):")
    start = time.time()
    results = []
    for d in dates:
        result = await process_date(d)
        results.append(result)
    sequential_time = time.time() - start
    logger.info(f"  100ê°œ ë‚ ì§œ ì²˜ë¦¬: {sequential_time:.2f}ì´ˆ")

    # 2. ë³‘ë ¬ ì²˜ë¦¬ (10ê°œ ì²­í¬)
    logger.info("\në³‘ë ¬ ì²˜ë¦¬ (ìµœì í™” ë°©ì‹):")
    start = time.time()

    # ë‚ ì§œë¥¼ 10ê°œ ì²­í¬ë¡œ ë¶„í• 
    chunk_size = 10
    chunks = [dates[i:i+chunk_size] for i in range(0, len(dates), chunk_size)]

    async def process_chunk(chunk):
        results = []
        for d in chunk:
            result = await process_date(d)
            results.append(result)
        return results

    # ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    tasks = [process_chunk(chunk) for chunk in chunks]
    chunk_results = await asyncio.gather(*tasks)

    # ê²°ê³¼ ë³‘í•©
    all_results = []
    for chunk_result in chunk_results:
        all_results.extend(chunk_result)

    parallel_time = time.time() - start
    logger.info(f"  100ê°œ ë‚ ì§œ ì²˜ë¦¬: {parallel_time:.2f}ì´ˆ")
    logger.info(f"  âš¡ ì†ë„ í–¥ìƒ: {sequential_time/parallel_time:.1f}ë°°")


async def main():
    """ë©”ì¸ ë°ëª¨ ì‹¤í–‰"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ ë°±í…ŒìŠ¤íŠ¸ ìµœì í™” ë°ëª¨")
    logger.info("=" * 80)
    logger.info("\nì„¸ ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ì‹œì—°í•©ë‹ˆë‹¤:")
    logger.info("1. Redis ìºì‹± - ì¤‘ë³µ ê³„ì‚° ë°©ì§€")
    logger.info("2. ì„ íƒì  íŒ©í„° ê³„ì‚° - í•„ìš”í•œ íŒ©í„°ë§Œ ê³„ì‚°")
    logger.info("3. ë³‘ë ¬ ì²˜ë¦¬ - ì—¬ëŸ¬ ë‚ ì§œ ë™ì‹œ ì²˜ë¦¬")

    # ê° ìµœì í™” ë°ëª¨ ì‹¤í–‰
    await demo_redis_caching()
    await demo_selective_calculation()
    await demo_parallel_processing()

    logger.info("\n" + "=" * 80)
    logger.info("âœ… ìµœì í™” ë°ëª¨ ì™„ë£Œ!")
    logger.info("=" * 80)
    logger.info("\nğŸ’¡ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ :")
    logger.info("  â€¢ 1ë…„ ë°±í…ŒìŠ¤íŠ¸: 180ì´ˆ â†’ 30-60ì´ˆ (3-6ë°° ê°œì„ )")
    logger.info("  â€¢ 5ë…„ ë°±í…ŒìŠ¤íŠ¸: 15ë¶„ â†’ 2-5ë¶„ (3-7ë°° ê°œì„ )")
    logger.info("  â€¢ ìºì‹œ íˆíŠ¸ì‹œ: ê±°ì˜ ì¦‰ì‹œ ì™„ë£Œ")
    logger.info("=" * 80 + "\n")


if __name__ == "__main__":
    asyncio.run(main())